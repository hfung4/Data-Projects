{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This function computes the missing percentage of each row or column. '''\n",
    "def missing_percentage(df, column):\n",
    "    if(column):  # missing value in each column\n",
    "        total = df.isnull().sum().sort_values(ascending = False)\n",
    "        percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)\n",
    "        return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n",
    "    else: # missing value in each row\n",
    "        total = df.isnull().sum(axis=1).sort_values(ascending=False)\n",
    "        percent = round(df.isnull().sum(axis=1).sort_values(ascending = False)/len(df.columns)*100,2)\n",
    "        return pd.concat([total, percent], axis=1, keys=['Total','Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function aggregate the item scores associated with each feature(take their mean) \n",
    "   to get the score for the feature (ex: WPI_challenge) '''\n",
    "def aggregate_col (df, feature, items):\n",
    "    df[feature] = df[items].mean(axis = 1, skipna= True)  # get item means and set it as the feature score\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This function performs initial data processing'''\n",
    "def process_data(year):\n",
    "    '''Import a file that specifies latent constructs within the dataset and their associated measured variables \n",
    "    This mapping is the ouptut of a measurement model was verified by a confirmatory factor analysis (CFA) \n",
    "    performed in STATA. '''\n",
    "    items = pd.read_excel(\"../data/aust_aps_items.xlsx\", encoding=\"latin-1\", sheet_name= str(year))\n",
    "    items.dropna(inplace=True) # drop all rows with NA\n",
    "    # Import the survey data\n",
    "    file_name= \"../data/raw_data_\" + str(year) + \".csv\"\n",
    "    raw = pd.read_csv(file_name, encoding=\"latin-1\")\n",
    "    # rename column names\n",
    "    raw.rename(columns={'AS':'org_size',\n",
    "                    'q1':'gender',\n",
    "                    'q2@': 'age'}, inplace=True)  \n",
    "    \n",
    "    '''I don't need ALL the items in the survey data, I will only keep the items that \n",
    "    relates to the latent constructs.'''\n",
    "    keep = [\"age\",\"gender\",\"org_size\"] + list(items[\"Q_Num\"])\n",
    "    unique_keep = list(set(keep))\n",
    "    unique_keep.sort()\n",
    "    df = raw.loc[:,unique_keep] \n",
    "    \n",
    "    ''' Data Processing\n",
    "    - Recode all items into numeric scores using pre-defined dictionaries in the helper file\n",
    "    - Deal with items with reversed coding\n",
    "    - Look at proportion of missing data for each item\n",
    "    - Remove respondents with more than 20% missing response'''\n",
    "    \n",
    "    # Map levels to numeric values\n",
    "    map_agree = {\"Strongly disagree\":1,\"Disagree\":2,\"Neither agree nor disagree\":3,\"Agree\":4,\"Strongly agree\":5}\n",
    "\n",
    "    map_satisfy = {\"Very dissatisfied\":1,\"Dissatisfied\":2,\"Neither satisfied or dissatisfied\":3,\"Satisfied\":4,\"Very satisfied\":5}\n",
    "\n",
    "    map_often ={\"Never\":1,\"Rarely\":2,\"Sometimes\":3,\"Often\":4,\"Always\":5}\n",
    "\n",
    "    map_often_rev ={\"Never\":5,\"Rarely\":4,\"Sometimes\":3,\"Often\":2,\"Always\":1}\n",
    "\n",
    "    map_somewhat ={\"Not at all\":1,\"Hardly at all\":2,\"Somewhat\":3,\"Quite a lot\":4,\"To a very great extent\":5}\n",
    "\n",
    "    map_leaving = {\"I want to stay working for my agency for at least the next three years\":0,\n",
    "                   \"I want to stay working for my agency for the next one to two years\":0,\n",
    "                   \"I want to leave my agency within the next 12 months but feel it will be unlikely in the current environment\":1,\n",
    "                   \"I want to leave my agency within the next 12 months\":1,\n",
    "                   \"I want to leave my agency as soon as possible\":1}\n",
    "    \n",
    "    \n",
    "    # Specify items that I need to recode\n",
    "    if year == 2019:\n",
    "        cols_to_move = ['age','gender','org_size','q38','q43a','q43b','q43c','q43d','q43f','q46']\n",
    "        rc_agree = 10\n",
    "        rc_satisfy = 'q38'\n",
    "        rc_often_rev = 'q43a'\n",
    "        rc_often = [\"q43b\",\"q43c\",\"q43d\",\"q43f\"]\n",
    "        rc_leaving = \"q46\"\n",
    "    elif year == 2018:\n",
    "        cols_to_move = ['age','gender','org_size','q33','q38a','q38b','q38c','q38d','q38f','q41']\n",
    "        rc_agree = 10\n",
    "        rc_satisfy = 'q33'\n",
    "        rc_often_rev = 'q38a'\n",
    "        rc_often = [\"q38b\",\"q38c\",\"q38d\",\"q38f\"]\n",
    "        rc_leaving = \"q41\"\n",
    "    else:\n",
    "        cols_to_move = ['age','gender','org_size','q41f','q41b','q41a','q41d','q36','q34h','q34e','q48']\n",
    "        rc_agree = 11\n",
    "        rc_satisfy = 'q36'\n",
    "        rc_often_rev = 'q41a'\n",
    "        rc_often = [\"q41b\",\"q41d\",\"q41f\"]\n",
    "        rc_leaving = \"q48\"\n",
    "        rc_somewhat = [\"q34h\",\"q34e\"] \n",
    "    \n",
    "    # Reorder columns    \n",
    "    new_cols = [col for col in list(df.columns) if col not in cols_to_move]\n",
    "    new_cols = cols_to_move + new_cols\n",
    "    df = df[new_cols]\n",
    "    \n",
    "    # recode all columns with \"Strongly disagree\"....\"Strongly agree\" to numerical scores \n",
    "    df.iloc[:,rc_agree:] = df.iloc[:,rc_agree:].replace(map_agree)\n",
    "    # recode satisfy\n",
    "    df[rc_satisfy]=df[rc_satisfy].replace(map_satisfy)\n",
    "    #recode often\n",
    "    df[rc_often_rev]=df[rc_often_rev].replace(map_often_rev)\n",
    "    df.loc[:,rc_often]=df.loc[:,rc_often].replace(map_often)\n",
    "    #recode leaving\n",
    "    df[rc_leaving]=df[rc_leaving].replace(map_leaving)\n",
    "    \n",
    "    if year == 2017:\n",
    "        #recode somewhat\n",
    "        df.loc[:,rc_somewhat]=df.loc[:,rc_somewhat].replace(map_somewhat)\n",
    "    \n",
    "    # replace blank cells with NaN\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    \n",
    "    # missing value count and percentage of each column\n",
    "    print(\"\")\n",
    "    print(\"********************\")\n",
    "    print(\"The missing value count and percentage of each item:\")\n",
    "    print(missing_percentage(df, True).sort_values(by=\"Percent\", ascending=False)[:10])\n",
    "\n",
    "    \n",
    "\n",
    " \n",
    "    # missing value count and percentage of each column\n",
    "    row_missing_count=missing_percentage(df, False)\n",
    "    # Only keep respondents with missing values less than 20%\n",
    "    keep_row = row_missing_count[row_missing_count[\"Percent\"]<20]\n",
    "    \n",
    "    # percentage of respondents retained\n",
    "    print(\"\")\n",
    "    print(\"********************\")\n",
    "    print(\"I kept {} % of respondent with missing value less than 20%\".format(int(len(keep_row)/len(row_missing_count)*100)))\n",
    "    \n",
    "    df=df.loc[keep_row.index,:]\n",
    "    df=df.sort_index(ascending=True)\n",
    "    \n",
    "    # one-hot encode age and org_size\n",
    "    df = pd.get_dummies(df, columns=[\"age\",\"org_size\",\"gender\"], drop_first=True) # drop the first level out of k level\n",
    "    \n",
    "    # print df shape\n",
    "    print(\"\")\n",
    "    print(\"********************\")\n",
    "    print(\"The shape of the Dataframe for all selected items is:{}\".format(df.shape))\n",
    "  \n",
    "    \n",
    "    '''Aggregating items scores to compute the construct scores\n",
    "    - Use item dataframe to build mapping tuples\n",
    "    - For simplicity, I will take the mean of items scores according to the tuples \n",
    "      to compute the score for each construct.\n",
    "    - In my project, I computed the scores for the latent construct using the measurement \n",
    "      model and the factor loading estimated from CFA. '''\n",
    "    \n",
    "    df_items = items[[\"Q_Num\",\"Factor\"]] # get a DataFrame with only Q_Num and Factor\n",
    "    features = list(df_items.Factor.unique()) # get a list of features\n",
    "    \n",
    "    agg_map=[]\n",
    "    for i in range(len(features)):\n",
    "        agg_map.append((features[i], [q for q in df_items[df_items.Factor==features[i]][\"Q_Num\"]])) \n",
    "    \n",
    "    for i in agg_map:\n",
    "        aggregate_col(df,i[0], i[1])\n",
    "        \n",
    "    df.drop(list(items[\"Q_Num\"]), axis=1, inplace=True)\n",
    "    \n",
    "    '''After computing the construct scores for each respondent by taking the mean of his/her associated items scores \n",
    "    (excluding NA), some constructs will still contain NA scores because: \n",
    "        1) the construct comprise of only 1 item and some respondents have missing values for that item, \n",
    "        2) all associated items of the construct for a respondent are NA\n",
    "    That said, all missing values are 1 % or lower.  Thus, I will remove respondent with NA score '''\n",
    "    print(\"\")\n",
    "    print(\"********************\")\n",
    "    print(missing_percentage(df, True).sort_values(by=\"Percent\", ascending=False).head(10))\n",
    "    \n",
    "    before = df.shape\n",
    "    df.dropna(inplace=True)\n",
    "    after = df.shape\n",
    "    print(\"\")\n",
    "    print(\"********************\")\n",
    "    print(\"I lose {} % of data after the removal of NA respondents\".format(abs((after[0]-before[0])/before[0])))\n",
    "    \n",
    "    # create a year column\n",
    "    df[\"year\"] = year\n",
    "    \n",
    "    df = df.sort_index(axis=1) # sort column by column names\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function that takes 1) df, 2) column, and return the count of each level of that factor '''\n",
    "def percent_value_counts(df,feature):\n",
    "    count = df.loc[:,feature].value_counts(dropna=False) # count is a Series\n",
    "    percent = round(df.loc[:,feature].value_counts(dropna=False, normalize=True)*100,2)\n",
    "    # Put the series count and percent in an output DataFrame\n",
    "    return pd.concat([count, percent], axis=1, keys=[\"count\", \"percent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function creates correlation matrix and outputs variables with correlation that is higher than 0.5'''\n",
    "def correlation_analysis(df, file_name):\n",
    "    # Generate a \"mask\" for the upper triangle\n",
    "    mask = np.zeros_like(df.corr(), dtype=np.bool)\n",
    "    corr = df.corr(method ='spearman') # correlation matrix\n",
    "    \n",
    "    #Look at highly correlated features pairs\n",
    "    highly_corr = corr.abs()[corr>0.5]\n",
    "    unstacked = highly_corr.unstack().sort_values(ascending=False).drop_duplicates()\n",
    "    unstacked.dropna(inplace=True)  # Drop all NA\n",
    "    unstacked = unstacked[unstacked!=1] # remove all diagonal elements in the cor matrix with corr = 1\n",
    "    \n",
    "    unstacked=unstacked.reset_index()\n",
    "    unstacked.columns=[\"Var1\",\"Var2\",\"Corr\"]\n",
    "    \n",
    "    top_corr_features= unstacked.sort_values(by=\"Corr\", ascending=False)\n",
    "    # Output to csv\n",
    "    top_corr_features.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function generates interaction terms by performing multiplication of two columns in a DataFrame'''\n",
    "def gen_interaction(df,x,y):  # x is a list of feature names, y is a single feature name \n",
    "    out = df.copy()\n",
    "    for feature in x:\n",
    "        values = out[feature] * out[y]\n",
    "        out[feature+\"_\"+y] = values\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
