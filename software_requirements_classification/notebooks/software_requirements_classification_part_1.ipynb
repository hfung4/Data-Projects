{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import pickle\n",
    "import pprint\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%run helpers.ipynb\n",
    "\n",
    "'''Visualization'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "''' Features'''\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "'''Estimators'''\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "'''Modelling'''\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score \n",
    "from sklearn.metrics import roc_auc_score, f1_score, roc_curve, auc, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the SecReq datasets\n",
    "CPN = pd.read_csv(\"../data/CPN.csv\", sep=\";\", names=[\"text\", \"labels\"])\n",
    "GPS = pd.read_csv(\"../data/GPS.csv\", sep=\";\", names=[\"text\", \"labels\"])\n",
    "ePurse = pd.read_csv(\"../data/ePurse_selective.csv\", sep=\";\", names=[\"text\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the nfr dataset\n",
    "nfr = pd.read_csv(\"../data/nfr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data in the labels column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210, 2), (178, 2), (124, 2), (625, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CPN.shape, GPS.shape, ePurse.shape, nfr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Internally to the CPN, a CNG transmitting pri...</td>\n",
       "      <td>sec,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Internally to the CPN, a CNG receiving privat...</td>\n",
       "      <td>sec,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The CNG shall detect the end of the life of th...</td>\n",
       "      <td>sec,,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A CPN-user attempting to invoke a CNG-mediate...</td>\n",
       "      <td>sec,,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The CNG shall implement an authentication fail...</td>\n",
       "      <td>sec,,,,,,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     labels\n",
       "0   Internally to the CPN, a CNG transmitting pri...   sec,,,,,\n",
       "1   Internally to the CPN, a CNG receiving privat...   sec,,,,,\n",
       "2  The CNG shall detect the end of the life of th...  sec,,,,,,\n",
       "3   A CPN-user attempting to invoke a CNG-mediate...  sec,,,,,,\n",
       "4  The CNG shall implement an authentication fail...  sec,,,,,,"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CPN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProjectID</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The system shall refresh the display every 60 ...</td>\n",
       "      <td>PE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The application shall match the color of the s...</td>\n",
       "      <td>LF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>If projected  the data must be readable.  On ...</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The product shall be available during normal ...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>If projected  the data must be understandable...</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProjectID                                               text labels\n",
       "0          1  The system shall refresh the display every 60 ...     PE\n",
       "1          1  The application shall match the color of the s...     LF\n",
       "2          1   If projected  the data must be readable.  On ...     US\n",
       "3          1   The product shall be available during normal ...      A\n",
       "4          1   If projected  the data must be understandable...     US"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [(\"CPN\",CPN), (\"GPS\",GPS), (\"ePurse\",ePurse), (\"nfr\",nfr)] # a list of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some basic cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean some of the wierd \",,,,,\" in labels\n",
    "def clean_labels(df):\n",
    "    df[\"labels\"] = df[\"labels\"].map(lambda x: re.sub(\",|\\\"\",\"\",str(x)))\n",
    "    df['labels'] = df['labels'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in datasets[:-1]:\n",
    "    clean_labels(df[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check to see if there is class imbalances for each dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_counts(df):\n",
    "    # count of each level\n",
    "    count = df[\"labels\"].value_counts(dropna=False)\n",
    "    # percentage of each level\n",
    "    percent = round(df[\"labels\"].value_counts(dropna=False, normalize=True)*100, 3)\n",
    "    \n",
    "    # put it into a DataFrame\n",
    "    return pd.concat([count,percent], axis=1, keys=[\"count\", \"percentage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPN:\n",
      "        count  percentage\n",
      "nonsec    167      79.524\n",
      "sec        41      19.524\n",
      "nan         2       0.952\n",
      "\n",
      "\n",
      "GPS:\n",
      "        count  percentage\n",
      "nonsec     94      52.809\n",
      "sec        45      25.281\n",
      "nan        37      20.787\n",
      "            1       0.562\n",
      "xyz         1       0.562\n",
      "\n",
      "\n",
      "ePurse:\n",
      "        count  percentage\n",
      "sec        83      66.935\n",
      "nonsec     41      33.065\n",
      "\n",
      "\n",
      "nfr:\n",
      "    count  percentage\n",
      "F     255       40.80\n",
      "US     67       10.72\n",
      "SE     66       10.56\n",
      "O      62        9.92\n",
      "PE     54        8.64\n",
      "LF     38        6.08\n",
      "SC     21        3.36\n",
      "A      21        3.36\n",
      "MN     17        2.72\n",
      "L      13        2.08\n",
      "FT     10        1.60\n",
      "PO      1        0.16\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df in datasets:\n",
    "    print(f\"{df[0]}:\")\n",
    "    pprint.pprint(level_counts(df[1]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's drop the rows with label as nan in CPN and blank and xyz in GPS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPN = CPN.loc[(CPN[\"labels\"] == \"sec\") | (CPN[\"labels\"] == \"nonsec\") , :]  # highly imbalanced: 80% nonsec, 20% sec\n",
    "GPS = GPS.loc[(GPS[\"labels\"] == \"sec\") | (GPS[\"labels\"] == \"nonsec\") , :]  # imbalanced: 67% nonsec, 32% sec\n",
    "ePurse = ePurse.loc[(ePurse[\"labels\"] == \"sec\") | (ePurse[\"labels\"] == \"nonsec\") , :] # imbalanced: 66% nonsec, 33% sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the nfr dataset, there are some labels with very little samples.  I can try to group all classes with less than 7% to a single \"other\" class\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = level_counts(nfr)\n",
    "others=list(temp[temp.percentage<7].index) # a list of classes that I want to group as \"others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfr.loc[nfr.labels.isin(others), \"labels\"]= \"others\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>255</td>\n",
       "      <td>40.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>121</td>\n",
       "      <td>19.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>67</td>\n",
       "      <td>10.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE</th>\n",
       "      <td>66</td>\n",
       "      <td>10.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>62</td>\n",
       "      <td>9.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PE</th>\n",
       "      <td>54</td>\n",
       "      <td>8.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count  percentage\n",
       "F         255       40.80\n",
       "others    121       19.36\n",
       "US         67       10.72\n",
       "SE         66       10.56\n",
       "O          62        9.92\n",
       "PE         54        8.64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_counts(nfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine CPN, GPS, ePurse datasets into a single SeqReq dataset.  We will perform binary classification on this dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SecReq = pd.concat([CPN,GPS,ePurse], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SecReq = SecReq.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The OPEN may also support additional selectio...</td>\n",
       "      <td>nonsec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If a supported scheme provider has a dispute r...</td>\n",
       "      <td>nonsec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiple public identities on the same termina...</td>\n",
       "      <td>nonsec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Protection mechanisms shall be implemented to ...</td>\n",
       "      <td>nonsec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two-way, or mutual, authentication, where the ...</td>\n",
       "      <td>sec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0   The OPEN may also support additional selectio...  nonsec\n",
       "1  If a supported scheme provider has a dispute r...  nonsec\n",
       "2  Multiple public identities on the same termina...  nonsec\n",
       "3  Protection mechanisms shall be implemented to ...  nonsec\n",
       "4  Two-way, or mutual, authentication, where the ...     sec"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SecReq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process software requirements text\n",
    "- Use spaCy to remove puncutation, non-alphanumric data, and elmmatize the text.\n",
    "- Train and apply first-order phrase model to join word pairs (get bigrams).\n",
    "- Train and apply second-order phrase model to join word triplets (get trigrams).\n",
    "- Remove stopwords.\n",
    "- Create tf-idf representations.\n",
    "\n",
    "Useful reference: https://towardsdatascience.com/turbo-charge-your-spacy-nlp-pipeline-551435b664ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Process text\n",
    "process_text(SecReq)\n",
    "process_text(nfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modelling\n",
    "- Learn combinations of tokens that together represents meaningful multi-word phrases (\"United States\", \"happy hour\")\n",
    "- Use gensim to develop phrase models phrase models are developed by examining all the words in The headlines and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance.\n",
    "- Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Useful reference: https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-7-phrase-modeling-doc2vec-592a8a996867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phraser model, get bigrams\n",
    "model_filepath = \"../models/secreq_bigrams_model\" # bigram model file path for SecReq\n",
    "get_bigrams(SecReq, model_filepath, True)\n",
    "\n",
    "model_filepath = \"../models/nfr_bigrams_model\" # bigram model file path for nfr\n",
    "get_bigrams(nfr, model_filepath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phraser model, get trigrams\n",
    "model_filepath = \"../models/secreq_trigrams_model\" # bigram model file path for SecReq\n",
    "get_trigrams(SecReq, model_filepath, True)\n",
    "\n",
    "model_filepath = \"../models/nfr_trigrams_model\" # bigram model file path for nfr\n",
    "get_trigrams(nfr, model_filepath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>The CNG Remote Management requires to manage a...</td>\n",
       "      <td>nonsec</td>\n",
       "      <td>[cng, remote, management, require, manage, spe...</td>\n",
       "      <td>[cng, remote_management, require, manage, spec...</td>\n",
       "      <td>[cng, remote_management, require, manage, spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>The GlobalPlatform Environment (OPEN) support...</td>\n",
       "      <td>nonsec</td>\n",
       "      <td>[globalplatform, environment, open, support, c...</td>\n",
       "      <td>[globalplatform, environment, open, support, c...</td>\n",
       "      <td>[globalplatform, environment, open, support, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>When applicable a Security Domain shall Verif...</td>\n",
       "      <td>sec</td>\n",
       "      <td>[applicable, security, domain, shall, verify, ...</td>\n",
       "      <td>[applicable, security_domain, shall, verify, l...</td>\n",
       "      <td>[applicable_security_domain, shall, verify, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>STB or media gateways should be equipped with ...</td>\n",
       "      <td>nonsec</td>\n",
       "      <td>[stb, medium, gateway, equipped, programmable,...</td>\n",
       "      <td>[stb_medium, gateway, equipped, programmable, ...</td>\n",
       "      <td>[stb_medium_gateway, equipped, programmable, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>A Global Services Application shall Hold the ...</td>\n",
       "      <td>sec</td>\n",
       "      <td>[global, services, application, shall, hold, g...</td>\n",
       "      <td>[global, services, application, shall, hold, g...</td>\n",
       "      <td>[global, services, application, shall, hold, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  labels  \\\n",
       "131  The CNG Remote Management requires to manage a...  nonsec   \n",
       "242   The GlobalPlatform Environment (OPEN) support...  nonsec   \n",
       "14    When applicable a Security Domain shall Verif...     sec   \n",
       "42   STB or media gateways should be equipped with ...  nonsec   \n",
       "451   A Global Services Application shall Hold the ...     sec   \n",
       "\n",
       "                                        processed_text  \\\n",
       "131  [cng, remote, management, require, manage, spe...   \n",
       "242  [globalplatform, environment, open, support, c...   \n",
       "14   [applicable, security, domain, shall, verify, ...   \n",
       "42   [stb, medium, gateway, equipped, programmable,...   \n",
       "451  [global, services, application, shall, hold, g...   \n",
       "\n",
       "                                               bigrams  \\\n",
       "131  [cng, remote_management, require, manage, spec...   \n",
       "242  [globalplatform, environment, open, support, c...   \n",
       "14   [applicable, security_domain, shall, verify, l...   \n",
       "42   [stb_medium, gateway, equipped, programmable, ...   \n",
       "451  [global, services, application, shall, hold, g...   \n",
       "\n",
       "                                              trigrams  \n",
       "131  [cng, remote_management, require, manage, spec...  \n",
       "242  [globalplatform, environment, open, support, c...  \n",
       "14   [applicable_security_domain, shall, verify, lo...  \n",
       "42   [stb_medium_gateway, equipped, programmable, o...  \n",
       "451  [global, services, application, shall, hold, g...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SecReq.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProjectID</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>5</td>\n",
       "      <td>The recycled parts audit report shall be retur...</td>\n",
       "      <td>PE</td>\n",
       "      <td>[recycled, part, audit, report, shall, return,...</td>\n",
       "      <td>[recycled_part, audit_report, shall, return, u...</td>\n",
       "      <td>[recycled_part, audit_report, shall, return, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>15</td>\n",
       "      <td>98% of the user workstations shall meet the mi...</td>\n",
       "      <td>O</td>\n",
       "      <td>[user, workstation, shall, meet, minimum, soft...</td>\n",
       "      <td>[user, workstation, shall, meet, minimum, soft...</td>\n",
       "      <td>[user, workstation, shall, meet, minimum, soft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>12</td>\n",
       "      <td>The product shall interface CSR and Datastream...</td>\n",
       "      <td>O</td>\n",
       "      <td>[product, shall, interface, csr, datastream, p...</td>\n",
       "      <td>[product, shall, interface, csr, datastream, p...</td>\n",
       "      <td>[product, shall, interface, csr, datastream, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>5</td>\n",
       "      <td>Product installations and upgrades shall be ha...</td>\n",
       "      <td>O</td>\n",
       "      <td>[product, installation, upgrade, shall, handle...</td>\n",
       "      <td>[product, installation, upgrade, shall, handle...</td>\n",
       "      <td>[product, installation, upgrade, shall, handle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3</td>\n",
       "      <td>Program Administrators/Nursing Staff Members ...</td>\n",
       "      <td>F</td>\n",
       "      <td>[program, administrators, nursing, staff, memb...</td>\n",
       "      <td>[program_administrators, nursing_staff, member...</td>\n",
       "      <td>[program_administrators_nursing_staff, member,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ProjectID                                               text labels  \\\n",
       "218          5  The recycled parts audit report shall be retur...     PE   \n",
       "615         15  98% of the user workstations shall meet the mi...      O   \n",
       "571         12  The product shall interface CSR and Datastream...      O   \n",
       "229          5  Product installations and upgrades shall be ha...      O   \n",
       "144          3   Program Administrators/Nursing Staff Members ...      F   \n",
       "\n",
       "                                        processed_text  \\\n",
       "218  [recycled, part, audit, report, shall, return,...   \n",
       "615  [user, workstation, shall, meet, minimum, soft...   \n",
       "571  [product, shall, interface, csr, datastream, p...   \n",
       "229  [product, installation, upgrade, shall, handle...   \n",
       "144  [program, administrators, nursing, staff, memb...   \n",
       "\n",
       "                                               bigrams  \\\n",
       "218  [recycled_part, audit_report, shall, return, u...   \n",
       "615  [user, workstation, shall, meet, minimum, soft...   \n",
       "571  [product, shall, interface, csr, datastream, p...   \n",
       "229  [product, installation, upgrade, shall, handle...   \n",
       "144  [program_administrators, nursing_staff, member...   \n",
       "\n",
       "                                              trigrams  \n",
       "218  [recycled_part, audit_report, shall, return, u...  \n",
       "615  [user, workstation, shall, meet, minimum, soft...  \n",
       "571  [product, shall, interface, csr, datastream, p...  \n",
       "229  [product, installation, upgrade, shall, handle...  \n",
       "144  [program_administrators_nursing_staff, member,...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfr.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variant of the nfr dataset in which there are only two classes: functional (F) and non-functional (NF)\n",
    "nfr_binary = nfr.copy()\n",
    "nfr_binary[\"labels\"].replace([\"others\",\"US\",\"SE\",\"O\",\"PE\"], \"NF\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels to numeric value\n",
    "encode_label(SecReq)\n",
    "encode_label(nfr)\n",
    "encode_label(nfr_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "secreq_texts = SecReq['trigrams'].astype('str')\n",
    "nfr_texts = nfr_binary_texts = nfr['trigrams'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification for the SecReq dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline for modelling**\n",
    "- Convert the transformed text (with trigrams) to a matrix of tf-idf weights (features)\n",
    "- The matrix would contain the tf-idf scores of each token t in each of the document in the corpus.\n",
    "- Perform TruncatedSVD to reduce the number of features-- this is important since our sample size is small, a large feature set will lead to overfitting.\n",
    "- Modelling: use RandomForest and Adaboost\n",
    "- Tune hyperparameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test harness\n",
    "X = secreq_texts\n",
    "y = SecReq['labels_num'].values \n",
    "\n",
    "# Train test split with stratified sampling for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify=y, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "models = {\n",
    "    'SVC': SVC(random_state=10),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=10),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(random_state=10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Parameters for GridSearchCV\n",
    "# Use a dictionary of dictionaries to set the param grid for each of the models\n",
    "'''\n",
    "SVC:\n",
    "C = the regularization parameter, penalize model complexity\n",
    "gamma = kernel coefficient for ‘rbf’ \n",
    "\n",
    "\n",
    "Random Forest:\n",
    "n_estimators = number of trees in the foreset\n",
    "max_features = max number of features considered for splitting a node\n",
    "max_depth = max number of levels in each decision tree\n",
    "min_samples_split = min number of data points placed in a node before the node is split\n",
    "min_samples_leaf = min number of data points allowed in a leaf node\n",
    "\n",
    "AdaBoost:\n",
    "n_estimators = number of estimators in the boosted ensemble to use.\n",
    "learning_rate= Learning rate shrinks the contribution of each classifier by learning_rate. \n",
    "'''       \n",
    "\n",
    "# Parameters for GridSearchCV\n",
    "params = {\n",
    "    \n",
    "    'SVC':  { \n",
    "        \"clf__C\" : [ 1, 10, 100, 1000], # [1,10,100,1000]\n",
    "        \"clf__gamma\" : [1,0.1,0.001,0.0001], # [1,0.1,0.001,0.0001]\n",
    "    },\n",
    "    \n",
    "    'RandomForestClassifier':  { \n",
    "        \"clf__n_estimators\" : [200, 400, 800, 1000, 1200], # [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800] \n",
    "        \"clf__max_depth\"      : [10,30,50,80,100],  # [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "        \"clf__min_samples_split\" : [2,5,10],  # [2, 5, 10]\n",
    "        \"clf__min_samples_leaf\" :   [1,2,4]   # [1, 2, 4]  \n",
    "    },\n",
    "    \n",
    "    'AdaBoostClassifier':  { \n",
    "        \"clf__n_estimators\" : [100, 500, 800, 1000], # [100, 500, 800, 1000]\n",
    "        \"clf__learning_rate\" : [0.01,0.05,0.1,0.5,1] # [0.01,0.05,0.1,0.5,1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    7.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__C': 10, 'clf__gamma': 1}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.8754312354312355\n",
      "Fitting 5 folds for each of 225 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 16.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__max_depth': 10, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 5, 'clf__n_estimators': 400}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.8085780885780884\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   37.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__learning_rate': 1, 'clf__n_estimators': 800}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.8632634032634033\n"
     ]
    }
   ],
   "source": [
    "df_secreq_tfidf = models_training(\"SecReq\",\"tfidf\", X_train, y_train, X_test, y_test, models, params, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>wv_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SecReq</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.871087</td>\n",
       "      <td>0.867809</td>\n",
       "      <td>0.869393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SecReq</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.852113</td>\n",
       "      <td>0.845335</td>\n",
       "      <td>0.828593</td>\n",
       "      <td>0.835603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SecReq</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.830441</td>\n",
       "      <td>0.766645</td>\n",
       "      <td>0.783181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset wv_type              model_name  accuracy_score  precision_score  \\\n",
       "0  SecReq   tfidf                     SVC        0.880282         0.871087   \n",
       "2  SecReq   tfidf      AdaBoostClassifier        0.852113         0.845335   \n",
       "1  SecReq   tfidf  RandomForestClassifier        0.816901         0.830441   \n",
       "\n",
       "   recall_score  f1_score  \n",
       "0      0.867809  0.869393  \n",
       "2      0.828593  0.835603  \n",
       "1      0.766645  0.783181  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_secreq_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts:**\n",
    "- I should select SVC since it requires the least training time and it has the best performance (on the test data)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification for the nfr_binary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test harness\n",
    "X = nfr_binary_texts\n",
    "y = nfr_binary['labels_num'].values \n",
    "\n",
    "# Train test split with stratified sampling for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify=y, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__C': 10, 'clf__gamma': 1}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.8695663531870428\n",
      "Fitting 5 folds for each of 225 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 19.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__max_depth': 10, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 5, 'clf__n_estimators': 1200}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.8032131661442007\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   46.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__learning_rate': 0.1, 'clf__n_estimators': 800}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.8352664576802507\n"
     ]
    }
   ],
   "source": [
    "df_nfr_binary_tfidf = models_training(\"nfr_binary\",\"tfidf\", X_train, y_train, X_test, y_test, models, params, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>wv_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nfr_binary</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.877660</td>\n",
       "      <td>0.875415</td>\n",
       "      <td>0.870539</td>\n",
       "      <td>0.872730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nfr_binary</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.835106</td>\n",
       "      <td>0.852122</td>\n",
       "      <td>0.810635</td>\n",
       "      <td>0.820870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nfr_binary</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.803191</td>\n",
       "      <td>0.811198</td>\n",
       "      <td>0.779630</td>\n",
       "      <td>0.787558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset wv_type              model_name  accuracy_score  \\\n",
       "0  nfr_binary   tfidf                     SVC        0.877660   \n",
       "1  nfr_binary   tfidf  RandomForestClassifier        0.835106   \n",
       "2  nfr_binary   tfidf      AdaBoostClassifier        0.803191   \n",
       "\n",
       "   precision_score  recall_score  f1_score  \n",
       "0         0.875415      0.870539  0.872730  \n",
       "1         0.852122      0.810635  0.820870  \n",
       "2         0.811198      0.779630  0.787558  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nfr_binary_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to represent software requirements with pre-trained Sentence Embeddings \n",
    "Useful Reference: https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Load KeyedVectors for the Google News word embeddings\n",
    "# Contains 3 million 300-D word embeddings trained from 100 billion words\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "# The vectors is loaded from an existing file on disk in the original Google’s word2vec C format as \n",
    "#a KeyedVectors instance\n",
    "googlew2v = KeyedVectors.load_word2vec_format('../embeddings/GoogleNews-vectors-negative300.bin.gz', \n",
    "                                                            binary=True) # C bin format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure. The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}. Note that with KeyVectors, I cannot continue to train the vectors.  However, KeyedVectors are smaller and need less RAM since htey don't need to store the model state that enbales training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the transformed requirements from each dataset a list of lists of tokens\n",
    "nfr_docs = nfr.trigrams.tolist()\n",
    "secreq_docs = SecReq.trigrams.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(doc):\n",
    "    # only keep tokens in each sentence if they are a vocab in googlew2v\n",
    "    tokens_in_embedding= [t for t in doc if t in googlew2v] \n",
    "    sent_embedding = np.average([googlew2v[token] for token in tokens_in_embedding], axis=0)\n",
    "    return sent_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sentence embeddings of each of the 625 requirements in the nfr dataset\n",
    "# For each requirements, I get the 300x1 word embedding for each token, and then take the average \n",
    "# to get a 300x1 sentence embedding\n",
    "sent_embeddings_nfr= [get_sentence_embeddings(doc) for doc in nfr_docs] # a list of 625 ndarrays of 300 elements\n",
    "sent_embeddings_nfr = np.concatenate(sent_embeddings_nfr, axis=0) # an ndarray of 625x300=187500 elements\n",
    "sent_embeddings_nfr = sent_embeddings_nfr.reshape((625,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embeddings_secreq = [get_sentence_embeddings(doc) for doc in secreq_docs] # a list of 471 ndarrays of 300 elements\n",
    "sent_embeddings_secreq = np.concatenate(sent_embeddings_secreq, axis=0) # an ndarray of 471x300=141300 elements\n",
    "sent_embeddings_secreq = sent_embeddings_secreq.reshape((471,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification for the SecReq dataset (with pre-trained word embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test harness\n",
    "X = sent_embeddings_secreq\n",
    "y = SecReq['labels_num'].values \n",
    "\n",
    "# Train test split with stratified sampling for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify=y, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__C': 100, 'clf__gamma': 0.1}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.8785081585081584\n",
      "Fitting 5 folds for each of 225 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 18.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__max_depth': 30, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'clf__n_estimators': 200}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.826946386946387\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__learning_rate': 1, 'clf__n_estimators': 1000}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.8542191142191143\n"
     ]
    }
   ],
   "source": [
    "df_secreq_wv = models_training(\"SecReq\",\"pre_trained_wv\", X_train, y_train, X_test, y_test, models, params, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>wv_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SecReq</td>\n",
       "      <td>pre_trained_wv</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.884188</td>\n",
       "      <td>0.887417</td>\n",
       "      <td>0.885748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SecReq</td>\n",
       "      <td>pre_trained_wv</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.858156</td>\n",
       "      <td>0.848201</td>\n",
       "      <td>0.852689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SecReq</td>\n",
       "      <td>pre_trained_wv</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.809490</td>\n",
       "      <td>0.783883</td>\n",
       "      <td>0.793281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset         wv_type              model_name  accuracy_score  \\\n",
       "2  SecReq  pre_trained_wv      AdaBoostClassifier        0.894366   \n",
       "0  SecReq  pre_trained_wv                     SVC        0.866197   \n",
       "1  SecReq  pre_trained_wv  RandomForestClassifier        0.816901   \n",
       "\n",
       "   precision_score  recall_score  f1_score  \n",
       "2         0.884188      0.887417  0.885748  \n",
       "0         0.858156      0.848201  0.852689  \n",
       "1         0.809490      0.783883  0.793281  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_secreq_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification for the nfr dataset (with pre-trained word embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test harness\n",
    "X = sent_embeddings_nfr\n",
    "y = nfr_binary['labels_num'].values \n",
    "\n",
    "# Train test split with stratified sampling for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify=y, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__C': 10, 'clf__gamma': 1}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.8490856844305121\n",
      "Fitting 5 folds for each of 225 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   38.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 23.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__max_depth': 10, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'clf__n_estimators': 1200}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.7827324973876697\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'clf__learning_rate': 1, 'clf__n_estimators': 500}\n",
      "\n",
      "The best accuracy score (on CV) are: 0.8009143155694881\n"
     ]
    }
   ],
   "source": [
    "df_nfr_binary_wv = models_training(\"nfr_binary\",\"pre_trained_wv\", X_train, y_train, X_test, y_test, models, params, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>wv_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nfr_binary</td>\n",
       "      <td>pre_trained_wv</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.877660</td>\n",
       "      <td>0.875415</td>\n",
       "      <td>0.870539</td>\n",
       "      <td>0.872730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nfr_binary</td>\n",
       "      <td>pre_trained_wv</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.856383</td>\n",
       "      <td>0.868280</td>\n",
       "      <td>0.836609</td>\n",
       "      <td>0.845916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nfr_binary</td>\n",
       "      <td>pre_trained_wv</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.781991</td>\n",
       "      <td>0.774073</td>\n",
       "      <td>0.777146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset         wv_type              model_name  accuracy_score  \\\n",
       "0  nfr_binary  pre_trained_wv                     SVC        0.877660   \n",
       "1  nfr_binary  pre_trained_wv  RandomForestClassifier        0.856383   \n",
       "2  nfr_binary  pre_trained_wv      AdaBoostClassifier        0.787234   \n",
       "\n",
       "   precision_score  recall_score  f1_score  \n",
       "0         0.875415      0.870539  0.872730  \n",
       "1         0.868280      0.836609  0.845916  \n",
       "2         0.781991      0.774073  0.777146  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nfr_binary_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
