{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "%run helpers.ipynb\n",
    "\n",
    "'''plots'''\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "'''keras'''\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Flatten, Dense, Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, GlobalMaxPooling1D\n",
    "from keras.layers import Dropout, concatenate\n",
    "from keras.layers.core import Reshape\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import regularizers\n",
    "\n",
    "'''Gensim'''\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "'''metrics'''\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, auc, roc_curve\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In general, pre-trained word embeddings are particularly useful on problems where there is very little training data (otherwise, I should train my word-embeddings on the corpus).  This is suitable to requirement classification since we usually don't have a large set of labelled data.  The labelling of software requirements need to be done manually by individual with domain expertise; thus even at Logapps, we only had a train dataset (labelled by trained interns in the previous summers) of about 5000 samples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "- Import the processed data from part 1 and part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SecReq = pd.read_csv(\"../processed_data/SecReq_processed.csv\")\n",
    "nfr = pd.read_csv(\"../processed_data/nfr_processed.csv\")\n",
    "nfr_binary = pd.read_csv(\"../processed_data/nfr_binary_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text\n",
    "- We no longer need to identify bigrams and trigrams using Phrase models ourselves since we are using CNN models.  We can define filters of size 2 and 3 (bigrams and trigrams) and during training, CNN will automatically (using backpropogation) extract/identify features (two or three word phrases) that are informative in predicting the class of the requirements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all functional class, I am only interested in classifying non-functional requirements to their subtypes\n",
    "nfr = nfr.loc[nfr.labels!=\"F\", :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SecReq = SecReq[[\"text\", \"labels_num\"]]\n",
    "nfr = nfr[[\"text\",\"labels_num\"]]\n",
    "nfr_binary = nfr_binary[[\"text\",\"labels_num\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SecReq.shape, nfr.shape, nfr_binary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have very small dataset (even smaller than the one I had at Logapps).  Hopefully, we can overcome this weakness with transfer-learning: using pre-trained word embeddings to learn \"generic low-level features\" and only focus on learning high level features with my small dataset.  We are very likely going to have overfitting problems with all our CNN models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the word2vec pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "execute = True\n",
    "\n",
    "if execute:\n",
    "    # Load KeyedVectors for the Google News word embeddings\n",
    "    # Contains 3 million 300-D word embeddings trained from 100 billion words\n",
    "\n",
    "    # Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "    # The vectors is loaded from an existing file on disk in the original Googleâ€™s word2vec C format as \n",
    "    # a KeyedVectors instance\n",
    "    word_vectors = KeyedVectors.load_word2vec_format('../embeddings/GoogleNews-vectors-negative300.bin.gz', \n",
    "                                                                binary=True) # C bin format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with binary classification with the SecReq dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process the text and build the embedding matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = SecReq[\"text\"]\n",
    "labels = SecReq[\"labels_num\"] \n",
    "\n",
    "X_train, X_val, y_train, y_val, embedding_matrix = keras_processing(texts, labels, max_words=1000, maxlen= 75) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "- As per Kim Yoon's \"Convolutional Neural Networks for Sentence Classification\", we will only use conv layer to extract features using filter_size 1,2 and 3.\n",
    "- The size of the filter is (filter_size, embeding_dim). If filter_size = 1, I will only extract unigram features; and if filter_size=2, I will extract bigrams features.\n",
    "- I will use 1-max pooling to get the max value of the feature map of each filter.\n",
    "- Thus, if I have 6 filters, I will get a 6x1 vector after the 1-max pooling.\n",
    "- Finally, I will have a fully connected layer and softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1] # number of words in each doc\n",
    "filter_sizes = [1,2] # filter size for conv layer 1, and for conv layer 2  (consider unigrams and bigrams)\n",
    "num_filters = 128 # number of filters for each conv layer\n",
    "drop = 0.5  # probability of dropping neurons\n",
    "\n",
    "vocabulary_size = embedding_matrix.shape[0]\n",
    "embedding_dim = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input to the embeddings layer for each doc is bascially an array of word index of length \"sequence_length\",\n",
    "# the length of each doc\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "# Emedding layer:\n",
    "    # Input: word index of shape (75,)\n",
    "    # output: a 3D tensor of shape (samples, sequence_length, embedding_dim) = (samples, 75, 300)\n",
    "embedding = embedding_layer(inputs)\n",
    "# reshape the output of the embedding layer to shape (75, 300, 1 )\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)  \n",
    "\n",
    "# conv layer 1 (use filter window size 1: extract unigrams)\n",
    "# input is the \"reshaped\" embedding matrix\n",
    "conv_0 = Conv2D(num_filters, # number of filters (50) \n",
    "                (filter_sizes[0], embedding_dim), # shape of the filter \n",
    "                activation='relu', # activation function\n",
    "                # Regularizers allow you to apply penalties on layer parameters or layer activity during optimization. \n",
    "                # These penalties are summed into the loss function that the network optimizes.\n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "# conv layer 2 (use filter window size 2: extract bigrams)\n",
    "# input is the \"reshaped\" embedding matrix\n",
    "conv_1 = Conv2D(num_filters, \n",
    "                (filter_sizes[1], embedding_dim),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "# Apply max pooling to the activation maps (100 of them) for EACH conv layer\n",
    "# sequence_length - filter_sizes[0] + 1 is the length of Each feature map\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0) # size of 1x100\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "\n",
    "# I get a feature from each of the 1-max pooling layer, so I have 2 features in total\n",
    "# I concat the features to get a 2 feature vector of size 2x100\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1], axis=1) \n",
    "flatten = Flatten()(merged_tensor) \n",
    "\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(2, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model class with takes a Keras.Input object and the outputs of the model as argument\n",
    "model = Model(inputs, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compile and train the network\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Use early stopping\n",
    "# callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                   epochs = 30,\n",
    "                   batch_size = 64,\n",
    "                   validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model performance over epochs\n",
    "train_acc = history.history[\"accuracy\"] # train accuracy of each of the 10 train epoch\n",
    "validation_acc = history.history[\"val_accuracy\"]\n",
    "train_loss = history.history[\"loss\"]\n",
    "validation_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1,len(train_acc)+1,1)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,6))\n",
    "axes[0].plot(epochs, train_acc, 'bo', label = \"Training acc\")\n",
    "axes[0].plot(epochs,validation_acc, 'b', label = \"Validation acc\")\n",
    "axes[0].set_title(\"Training and validation accuracy\")\n",
    "axes[0].set_ylabel(\"accuracy\")\n",
    "axes[0].set_xlabel(\"epoch\")\n",
    "\n",
    "\n",
    "axes[1].plot(epochs, train_loss, 'bo', label = \"Training loss\")\n",
    "axes[1].plot(epochs,validation_loss, 'b', label = \"Validation loss\")\n",
    "axes[1].set_title(\"Training and validation loss\")\n",
    "axes[1].set_ylabel(\"loss\")\n",
    "axes[1].set_xlabel(\"epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists of model name and model performance metrics\n",
    "model_name, acc_score_list, precision_score_list, recall_score_list, f1_score_list = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = model.predict(X_val)\n",
    "# Get the predicted class by taking the max probability\n",
    "y_pred_classes = np.argmax(y_pred,axis=1)\n",
    "# Get the predicted classes of y_val\n",
    "y_val_classes = [np.argmax(y, axis=None, out=None) for y in y_val]\n",
    "y_val_classes = np.asarray(y_val_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model performance lists\n",
    "\n",
    "# Model performance\n",
    "acc_score_list.append(accuracy_score(y_val_classes, y_pred_classes))\n",
    "# compute precision of each class and take average\n",
    "precision_score_list.append(precision_score(y_val_classes, y_pred_classes)) \n",
    "recall_score_list.append(recall_score(y_val_classes, y_pred_classes))\n",
    "f1_score_list.append(f1_score(y_val_classes, y_pred_classes))\n",
    "# model name\n",
    "model_name.append(\"CNN (SeqReq)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model name: CNN(SeqReq) \\n \\\n",
    "      accuracy: {accuracy_score(y_val_classes, y_pred_classes)} \\n \\\n",
    "      precision: {precision_score(y_val_classes, y_pred_classes)} \\n \\\n",
    "      recall: {recall_score(y_val_classes, y_pred_classes)} \\n \\\n",
    "      f1: {f1_score(y_val_classes, y_pred_classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Binary classification with the nfr_binary dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process the text and build the embedding matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfr_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts = nfr_binary[\"text\"]\n",
    "labels = nfr_binary[\"labels_num\"] \n",
    "\n",
    "X_train, X_val, y_train, y_val, embedding_matrix = keras_processing(texts, labels, max_words=1000, maxlen= 75) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1] # number of words in each doc\n",
    "filter_sizes = [1,2] # filter size for conv layer 1, and for conv layer 2  (consider unigrams and bigrams)\n",
    "num_filters = 64 # number of filters for each conv layer\n",
    "drop = 0.5  # probability of dropping neurons\n",
    "\n",
    "vocabulary_size = embedding_matrix.shape[0]\n",
    "embedding_dim = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)  \n",
    "conv_0 = Conv2D(num_filters, \n",
    "                (filter_sizes[0], embedding_dim),  \n",
    "                activation='relu', \n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, \n",
    "                (filter_sizes[1], embedding_dim),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0) # size of 1x100\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1], axis=1) \n",
    "flatten = Flatten()(merged_tensor) \n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(2, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model class with takes a Keras.Input object and the outputs of the model as argument\n",
    "model = Model(inputs, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compile and train the network\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Use early stopping\n",
    "# callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                   epochs = 100,\n",
    "                   batch_size = 64,\n",
    "                   validation_data = (X_val, y_val),\n",
    "                   verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model performance over epochs\n",
    "train_acc = history.history[\"accuracy\"] # train accuracy of each of the 10 train epoch\n",
    "validation_acc = history.history[\"val_accuracy\"]\n",
    "train_loss = history.history[\"loss\"]\n",
    "validation_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1,len(train_acc)+1,1)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,6))\n",
    "axes[0].plot(epochs, train_acc, 'bo', label = \"Training acc\")\n",
    "axes[0].plot(epochs,validation_acc, 'b', label = \"Validation acc\")\n",
    "axes[0].set_title(\"Training and validation accuracy\")\n",
    "axes[0].set_ylabel(\"accuracy\")\n",
    "axes[0].set_xlabel(\"epoch\")\n",
    "\n",
    "\n",
    "axes[1].plot(epochs, train_loss, 'bo', label = \"Training loss\")\n",
    "axes[1].plot(epochs,validation_loss, 'b', label = \"Validation loss\")\n",
    "axes[1].set_title(\"Training and validation loss\")\n",
    "axes[1].set_ylabel(\"loss\")\n",
    "axes[1].set_xlabel(\"epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = model.predict(X_val)\n",
    "# Get the predicted class by taking the max probability\n",
    "y_pred_classes = np.argmax(y_pred,axis=1)\n",
    "# Get the predicted classes of y_val\n",
    "y_val_classes = [np.argmax(y, axis=None, out=None) for y in y_val]\n",
    "y_val_classes = np.asarray(y_val_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model performance lists\n",
    "\n",
    "# Model performance\n",
    "acc_score_list.append(accuracy_score(y_val_classes, y_pred_classes))\n",
    "# compute precision of each class and take average\n",
    "precision_score_list.append(precision_score(y_val_classes, y_pred_classes)) \n",
    "recall_score_list.append(recall_score(y_val_classes, y_pred_classes))\n",
    "f1_score_list.append(f1_score(y_val_classes, y_pred_classes))\n",
    "# model name\n",
    "model_name.append(\"CNN (nfr_binary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model name: CNN(nfr_binary) \\n \\\n",
    "      accuracy: {accuracy_score(y_val_classes, y_pred_classes)} \\n \\\n",
    "      precision: {precision_score(y_val_classes, y_pred_classes)} \\n \\\n",
    "      recall: {recall_score(y_val_classes, y_pred_classes)} \\n \\\n",
    "      f1: {f1_score(y_val_classes, y_pred_classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification with the nfr dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process the text and build the embedding matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nfr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = nfr[\"text\"]\n",
    "labels = nfr[\"labels_num\"] -1 # shift all labels_num to the left\n",
    "\n",
    "X_train, X_val, y_train, y_val, embedding_matrix = keras_processing(texts, labels, max_words=1000, maxlen= 75) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1] # number of words in each doc\n",
    "filter_sizes = [1,2] # filter size for conv layer 1, and for conv layer 2  (consider unigrams and bigrams)\n",
    "num_filters = 64 # number of filters for each conv layer\n",
    "drop = 0.5  # probability of dropping neurons\n",
    "\n",
    "vocabulary_size = embedding_matrix.shape[0]\n",
    "embedding_dim = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)  \n",
    "conv_0 = Conv2D(num_filters, \n",
    "                (filter_sizes[0], embedding_dim),  \n",
    "                activation='relu', \n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, \n",
    "                (filter_sizes[1], embedding_dim),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0) # size of 1x100\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1], axis=1) \n",
    "flatten = Flatten()(merged_tensor) \n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(6, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model class with takes a Keras.Input object and the outputs of the model as argument\n",
    "model = Model(inputs, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compile and train the network\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Use early stopping\n",
    "# callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                   epochs = 140,\n",
    "                   batch_size = 128,\n",
    "                   validation_data = (X_val, y_val),\n",
    "                   verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model performance over epochs\n",
    "train_acc = history.history[\"accuracy\"] # train accuracy of each of the 10 train epoch\n",
    "validation_acc = history.history[\"val_accuracy\"]\n",
    "train_loss = history.history[\"loss\"]\n",
    "validation_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1,len(train_acc)+1,1)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,6))\n",
    "axes[0].plot(epochs, train_acc, 'bo', label = \"Training acc\")\n",
    "axes[0].plot(epochs,validation_acc, 'b', label = \"Validation acc\")\n",
    "axes[0].set_title(\"Training and validation accuracy\")\n",
    "axes[0].set_ylabel(\"accuracy\")\n",
    "axes[0].set_xlabel(\"epoch\")\n",
    "\n",
    "\n",
    "axes[1].plot(epochs, train_loss, 'bo', label = \"Training loss\")\n",
    "axes[1].plot(epochs,validation_loss, 'b', label = \"Validation loss\")\n",
    "axes[1].set_title(\"Training and validation loss\")\n",
    "axes[1].set_ylabel(\"loss\")\n",
    "axes[1].set_xlabel(\"epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = model.predict(X_val)\n",
    "# Get the predicted class by taking the max probability\n",
    "y_pred_classes = np.argmax(y_pred,axis=1)\n",
    "# Get the predicted classes of y_val\n",
    "y_val_classes = [np.argmax(y, axis=None, out=None) for y in y_val]\n",
    "y_val_classes = np.asarray(y_val_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model performance lists\n",
    "acc = accuracy_score(y_val_classes, y_pred_classes)\n",
    "precision = precision_score(y_val_classes, y_pred_classes, average=\"macro\")\n",
    "recall = recall_score(y_val_classes, y_pred_classes,average=\"macro\")\n",
    "f1 = f1_score(y_val_classes, y_pred_classes,average=\"macro\")\n",
    "\n",
    "# Model performance\n",
    "acc_score_list.append(acc)\n",
    "# compute precision of each class and take average\n",
    "precision_score_list.append(precision) \n",
    "recall_score_list.append(recall)\n",
    "f1_score_list.append(f1)\n",
    "# model name\n",
    "model_name.append(\"CNN (nfr_multiclass)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model name: CNN(nfr_multiclass) \\n \\\n",
    "      accuracy: {acc} \\n \\\n",
    "      precision: {precision} \\n \\\n",
    "      recall: {recall} \\n \\\n",
    "      f1: {f1}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The diagonal elements are correctly predicted by the model.  I see small off-diagonal values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\"LF\":0, \"O\":1, \"PE\":2, \"SE\":3, \"US\":4, \"others\":5} # mapping of labels and label_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=list(label_dict.keys()),  yticklabels=list(label_dict.keys()))\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.savefig(\"../outputs/multi_class_classification_cnn_cm.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val_classes, y_pred_classes, target_names=list(label_dict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts**\n",
    "\n",
    "The CNN model outperforms the SVC model for almost all classes (except for others). Despite the fact that CNN models require a large set of training data, we were able to compensate for this weakness with pre-trained word-embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "- Compare CNN against \"traditional\" statistical learning methods for:\n",
    "    - SecReq Binary Classification (Security-related /Non-Security-related)\n",
    "    - nfr binary classification (Functional requirements/ Non-functional requirements)\n",
    "    - nfr multiclass classification (6 types of non-functional requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import results of traditional statistical learning methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_results_trad = pd.read_csv(\"../outputs/binary_classification_results_traditional.csv\")\n",
    "multiclass_results_trad = pd.read_csv(\"../outputs/multiclass_results_traditional.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_results_trad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_results_cnn = pd.DataFrame({\"dataset\":[\"SecReq\", \"nfr_binary\"],\n",
    "                                   \"wv_type\":[\"pre_trained_wv\", \"pre_trained_wv\"], \n",
    "                                   \"model_name\":[\"CNN\", \"CNN\"],\n",
    "                                   \"accuracy_score\":[acc_score_list[0], acc_score_list[1]],\n",
    "                                   \"precision_score\":[precision_score_list[0], precision_score_list[1]],\n",
    "                                   \"recall_score\":[recall_score_list[0], recall_score_list[1]],\n",
    "                                   \"f1_score\":[f1_score_list[0], f1_score_list[1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_results_cnn =  pd.DataFrame({\"dataset\":[\"nfr\"],\n",
    "                                   \"wv_type\":[\"pre_trained_wv\"], \n",
    "                                   \"model_name\":[\"CNN\"],\n",
    "                                   \"accuracy_score\":[acc_score_list[2]],\n",
    "                                   \"precision_score\":[precision_score_list[2]],\n",
    "                                   \"recall_score\":[recall_score_list[2]],\n",
    "                                   \"f1_score\":[f1_score_list[2]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plots- Binary classification model comparisions\n",
    "fig, axes = plt.subplots(1,2,figsize=(16,8))\n",
    "sns.barplot(x=\"model_name\", y=\"f1_score\", hue=\"wv_type\", data=secreq_binary_results, ax=axes[0])\n",
    "sns.barplot(x=\"model_name\", y=\"f1_score\", hue=\"wv_type\", data=nfr_binary_results, ax=axes[1])\n",
    "\n",
    "axes[0].title.set_text('SeqReq Dataset')\n",
    "axes[1].title.set_text('nfr binary Dataset')\n",
    "\n",
    "plt.savefig(\"../outputs/binary_classification_results.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plots- Multiclass classification model comparisions\n",
    "fig,ax = plt.subplots(figsize=(10,6))\n",
    "sns.barplot(x=\"model_name\", y=\"f1_score\", data=model_comparison_df, axes=ax)\n",
    "\n",
    "ax.set_title('nfr dataset (multiclass non-functional requirements classification)')\n",
    "\n",
    "plt.savefig(\"../outputs/final_model_comparison/multi_class_classification_results_final.jpeg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
