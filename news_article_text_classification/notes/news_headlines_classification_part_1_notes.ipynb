{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this project, I will process labelled news headlines, perform topic modelling, and then build predictive models to classify news headlinesinto different domains: Business, SciTech, Sports, World.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import itertools as it\n",
    "\n",
    "'''Visualization'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "''' Features'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "'''Estimators'''\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "'''Modelling'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_curve, auc, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\": \"Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",\"annotation\":{\"notes\":\"\",\"label\":[\"Business\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1521027375000,\"last_updated_at\":1521027375000,\"sec_taken\":0,\"last_updated_by\":\"nlYZXxNBQefF2u9VX52CdONFp0C3\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at what the first headline looks like\n",
    "with open(\"news_article_data.json\") as file:\n",
    "    first_record = file.readline()\n",
    "\n",
    "print(first_record)  # two fields are of interest: \"content\" and \"annotation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write all headlines that are labelled \"Business\" in a text file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "scratch_directory = \"scratch\"\n",
    "\n",
    "# create a scratch directory if one doesn't already exist\n",
    "try:\n",
    "    os.mkdir(scratch_directory)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# path for our text file that contains all the business headlines\n",
    "business_txt_file_path = os.path.join(scratch_directory,\"business_headlines_all.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of news articles written into business_headlines_all.txt is 1900\n"
     ]
    }
   ],
   "source": [
    "execute = True   # can turn this flag to True if I want to run this cell\n",
    "\n",
    "if execute:\n",
    "    business_count = 0 # count the number of business headlines\n",
    "    \n",
    "    # create and open a new file in write mode\n",
    "    with open(business_txt_file_path,'w') as business_txt_file:\n",
    "        \n",
    "        \n",
    "        # open the news headline data file in read mode\n",
    "        with open (\"news_article_data.json\",'r') as headlines_file:\n",
    "            \n",
    "            # loop through all rows in the headline file and convert each row into a dict\n",
    "            for line in headlines_file:\n",
    "                headline = json.loads(line)\n",
    "                \n",
    "                # if the headline is not labelled \"Business\", go back to the start of the loop for the next headline\n",
    "                if headline['annotation']['label'][0] !=\"Business\":\n",
    "                    continue\n",
    "                \n",
    "                # write the news article that is labelled \"Business\" in the new file\n",
    "                business_txt_file.write(headline['content']+'\\n' + '\\n')\n",
    "                business_count +=1 # increment count\n",
    "    \n",
    "    print(\"The number of news articles written into business_headlines_all.txt is {}\".format(business_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Read in each headline and convert it to a Python dict, \n",
    "# get the text of each headline, and write it into a new text file that  \n",
    "# contains  one headline per line in the file.\n",
    "\n",
    "scratch_directory = os.path.join('scratch')\n",
    "\n",
    "# create a scratch directory if one doesn't already exist\n",
    "try:\n",
    "    os.mkdir(scratch_directory)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "headline_txt_filepath = os.path.join(scratch_directory, 'headline_text_all.txt') # name of the text file that contains all headline text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_count = 0 #  init headline counter\n",
    "label = [] # a list for the headline labels\n",
    "\n",
    "# create and open a new file in write mode\n",
    "with open(headline_txt_filepath, 'w') as headline_txt_file:\n",
    "    \n",
    "    # open \"news_article_data.json\" in read mode (the original data file)\n",
    "    with open (\"news_article_data.json\", 'r') as headline_json_file:\n",
    "        \n",
    "        # convert each json record into dict and write the headline text into the new file\n",
    "        for line in headline_json_file:\n",
    "            headline = json.loads(line)\n",
    "            headline_txt_file.write(headline['content'] + '\\n')\n",
    "            headline_count +=1\n",
    "            \n",
    "            #Get label\n",
    "            label.append(headline['annotation']['label'][0])  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7600, 7600)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label), headline_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grab a sample headline to \"play\" with and try out the functionalities of spaCy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bea Arthur sparked a security scare at Logan Airport in Boston this week when she tried to board a Cape Air flight with a pocketknife in her handbag.    The \"Golden Girls\" star, now 81, was flagged by a Transportation Security Administration agent, who discovered the knife - a strict no-no following 9/11.    \"She started yelling that it wasn't hers and said 'The terrorists put it there,' \" a fellow passenger said. \"She kept yelling about the 'terrorists, the terrorists, the terrorists.' \"    After the blade was confiscated, Arthur took a keyring from her bag and told the agent it belonged to the \"terrorists,\" before throwing it at them.  - via philly.com\n"
     ]
    }
   ],
   "source": [
    "headline_num = 789\n",
    "\n",
    "with open(\"news_article_data.json\") as file:\n",
    "    # convert the json record to a Python dict. \"data\" is a list of dicts\n",
    "    data=[json.loads(line) for line in file]\n",
    "    \n",
    "sample_headline = data[headline_num]['content']\n",
    "print(sample_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_headline = nlp(sample_headline) # apply spaCy nlp models to the sample headline (ex: tagger, parser, ner )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bea Arthur sparked a security scare at Logan Airport in Boston this week when she tried to board a Cape Air flight with a pocketknife in her handbag.    The \"Golden Girls\" star, now 81, was flagged by a Transportation Security Administration agent, who discovered the knife - a strict no-no following 9/11.    \"She started yelling that it wasn't hers and said 'The terrorists put it there,' \" a fellow passenger said. \"She kept yelling about the 'terrorists, the terrorists, the terrorists.' \"    After the blade was confiscated, Arthur took a keyring from her bag and told the agent it belonged to the \"terrorists,\" before throwing it at them.  - via philly.com\n"
     ]
    }
   ],
   "source": [
    "print(parsed_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks the same so far!  But spaCy already performed all the sentence detection, tokenization, normalization etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator at 0x1ad613422c8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_headline.sents # sentencize the sample headline, I get a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence number:0\n",
      "Bea Arthur sparked a security scare at Logan Airport in Boston this week when she tried to board a Cape Air flight with a pocketknife in her handbag.    \n",
      "\n",
      "Sentence number:1\n",
      "The \"Golden Girls\" star, now 81, was flagged by a Transportation Security Administration agent, who discovered the knife - a strict no-no following 9/11.    \n",
      "\n",
      "Sentence number:2\n",
      "\"She started yelling that it wasn't hers and said 'The terrorists put it there,' \" a fellow passenger said.\n",
      "\n",
      "Sentence number:3\n",
      "\"She kept yelling about the 'terrorists, the terrorists, the terrorists.' \"    \n",
      "\n",
      "Sentence number:4\n",
      "After the blade was confiscated, Arthur took a keyring from her bag and told the agent it belonged to the \"terrorists,\" before throwing it at them.  \n",
      "\n",
      "Sentence number:5\n",
      "- via philly.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(parsed_headline.sents):\n",
    "    print(\"Sentence number:{}\".format(i))\n",
    "    print(text)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization, lemmatization, and token shape analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text = [token.text for token in parsed_headline] \n",
    "token_lemma = [token.lemma_ for token in parsed_headline] \n",
    "token_shape = [token.shape_ for token in parsed_headline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token text</th>\n",
       "      <th>token lemma</th>\n",
       "      <th>token shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bea</td>\n",
       "      <td>Bea</td>\n",
       "      <td>Xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arthur</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sparked</td>\n",
       "      <td>spark</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>security</td>\n",
       "      <td>security</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>scare</td>\n",
       "      <td>scare</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Logan</td>\n",
       "      <td>Logan</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Airport</td>\n",
       "      <td>Airport</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token text token lemma token shape\n",
       "0        Bea         Bea         Xxx\n",
       "1     Arthur      Arthur       Xxxxx\n",
       "2    sparked       spark        xxxx\n",
       "3          a           a           x\n",
       "4   security    security        xxxx\n",
       "5      scare       scare        xxxx\n",
       "6         at          at          xx\n",
       "7      Logan       Logan       Xxxxx\n",
       "8    Airport     Airport       Xxxxx\n",
       "9         in          in          xx"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp= pd.DataFrame(zip(token_text, token_lemma, token_shape),\n",
    "            columns= ['token text', 'token lemma', 'token shape'])\n",
    "\n",
    "temp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCY also performs part of speech tagging (detects the verb, noun, proper noun etc... in a sentence)**\n",
    "- Noun (N)- Daniel, London, table, dog, teacher, pen, city, happiness, hope\n",
    "- Verb (V)- go, speak, run, eat, play, live, walk, have, like, are, is\n",
    "- Adjective(ADJ)- big, happy, green, young, fun, crazy, three\n",
    "- Adverb(ADV)- slowly, quietly, very, always, never, too, well, tomorrow\n",
    "- Preposition (P)- at, on, in, from, with, near, between, about, under\n",
    "- Conjunction (CON)- and, or, but, because, so, yet, unless, since, if\n",
    "- Pronoun(PRO)- I, you, we, they, he, she, it, me, us, them, him, her, this\n",
    "- Interjection (INT)- Ouch! Wow! Great! Help! Oh! Hey! Hi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>she</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>'</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>scare</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>her</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>at</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Transportation</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>strict</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>kept</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token_text part_of_speech\n",
       "54                a            DET\n",
       "14              she           PRON\n",
       "103               '          PUNCT\n",
       "5             scare           NOUN\n",
       "26              her            DET\n",
       "134              at            ADP\n",
       "44   Transportation          PROPN\n",
       "55           strict            ADJ\n",
       "90             kept           VERB\n",
       "22             with            ADP"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pos = [token.pos_ for token in parsed_headline]\n",
    "temp = pd.DataFrame(\n",
    "    zip(token_text, token_pos),\n",
    "    columns=['token_text', 'part_of_speech']\n",
    "    )\n",
    "\n",
    "temp.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name Entity Recognition (NER)**\n",
    "- A name entity is a \"real world object\" that is assigned a name.  For example:\n",
    "    - A person\n",
    "    - A country\n",
    "    - A product\n",
    "    - A book title\n",
    "        \n",
    "- If my text data is a chapter of a fiction, and my task is to find out the character that appeared the most in that chapter, then I need NER.\n",
    "- First, I need to identify the names in a sentence.\n",
    "- Then I need to get a list of all the unique names that appear in all the sentences in the chapter.\n",
    "- Finally, I count the number of times that each name appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bea Arthur sparked a security scare at Logan Airport in Boston this week when she tried to board a Cape Air flight with a pocketknife in her handbag.    The \"Golden Girls\" star, now 81, was flagged by a Transportation Security Administration agent, who discovered the knife - a strict no-no following 9/11.    \"She started yelling that it wasn't hers and said 'The terrorists put it there,' \" a fellow passenger said. \"She kept yelling about the 'terrorists, the terrorists, the terrorists.' \"    After the blade was confiscated, Arthur took a keyring from her bag and told the agent it belonged to the \"terrorists,\" before throwing it at them.  - via philly.com"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bea Arthur,\n",
       " Logan Airport,\n",
       " Boston,\n",
       " this week,\n",
       " Cape Air,\n",
       " The \"Golden Girls\" star,\n",
       " 81,\n",
       " Transportation Security Administration,\n",
       " 9/11,\n",
       " Arthur)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_headline.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(parsed_headline.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: Bea Arthur - PERSON\n",
      "Entity 2: Logan Airport - FAC\n",
      "Entity 3: Boston - GPE\n",
      "Entity 4: this week - DATE\n",
      "Entity 5: Cape Air - PRODUCT\n",
      "Entity 6: The \"Golden Girls\" star - FAC\n",
      "Entity 7: 81 - DATE\n",
      "Entity 8: Transportation Security Administration - ORG\n",
      "Entity 9: 9/11 - CARDINAL\n",
      "Entity 10: Arthur - PERSON\n"
     ]
    }
   ],
   "source": [
    "for i, entity in enumerate(parsed_headline.ents):\n",
    "    print(\"Entity {}: {} - {}\".format(i+1, entity, entity.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token-level attributes**\n",
    "- Whether or not a token is a:\n",
    "    - stopword?\n",
    "    - punctuation?\n",
    "    - whitespace?\n",
    "    - a number?\n",
    "    - is part of spaCy's default vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_attributes = [(\n",
    "        token.text,  # text\n",
    "        token.prob, # log prob (frequency of word)\n",
    "        token.is_stop, # stop word?\n",
    "        token.is_punct, # a puncuation?\n",
    "        token.is_space, # a whitespace?\n",
    "        token.like_num, # a number?\n",
    "        token.is_oov # not a spaCy vocab?\n",
    "        ) for token in parsed_headline\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(token_attributes,\n",
    "       columns=['text', 'log_probability',\n",
    "        'stop?', 'punctuation?',\n",
    "        'whitespace?', 'number?',\n",
    "        'out of vocab.?']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>before</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>throwing</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>it</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>at</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>them</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>.</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td></td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>-</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>via</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>philly.com</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text  log_probability stop? punctuation? whitespace? number?  \\\n",
       "131      before            -20.0   Yes                                    \n",
       "132    throwing            -20.0                                          \n",
       "133          it            -20.0   Yes                                    \n",
       "134          at            -20.0   Yes                                    \n",
       "135        them            -20.0   Yes                                    \n",
       "136           .            -20.0                Yes                       \n",
       "137                        -20.0                            Yes           \n",
       "138           -            -20.0                Yes                       \n",
       "139         via            -20.0   Yes                                    \n",
       "140  philly.com            -20.0                                          \n",
       "\n",
       "    out of vocab.?  \n",
       "131            Yes  \n",
       "132            Yes  \n",
       "133            Yes  \n",
       "134            Yes  \n",
       "135            Yes  \n",
       "136            Yes  \n",
       "137            Yes  \n",
       "138            Yes  \n",
       "139            Yes  \n",
       "140            Yes  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.loc[:, 'stop?':'out of vocab.?'] = (\n",
    "    temp.loc[:, 'stop?':'out of vocab.?']\n",
    "    .applymap(lambda x: 'Yes' if x else '')\n",
    "    )\n",
    "                                               \n",
    "temp.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "\n",
    "- The intended interpretation of oov is, \"tokens that don't have a meaningful probability value are words that are not in the vocab.\"\n",
    "\n",
    "- oov is not that useful in the _sm models, and in general might not work well in data packs if there wasn't a big frequency count used to build the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text\n",
    "- Use spaCy to remove punctuation and lemmatize the text.\n",
    "- Train and apply first-order phrase model to join word pairs.\n",
    "- Train and apply second-order phrase model to join longer phrases.\n",
    "- Remove stopwords.\n",
    "- Create a bag-of-words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# This function helps eliminate tokens that are punctutation or whitespace.\n",
    "def punct_space(token):\n",
    "\n",
    "    '''\n",
    "    NOTE: \n",
    "\n",
    "    Unlike verbs and common nouns, there is no clear base form of a personal pronoun. \n",
    "    It is not clear if the lemma of \"me\" sould be \"I\", or whether a person is normalized as \"it\" or \"she\" or \"he\"? \n",
    "    spaCy's solution is to introduce a novel symbol, -PRON-, which is used as the lemma for all personal pronouns.\n",
    "    '''\n",
    "\n",
    "    # I will return the actual text if I encounter a pronoun instead of using spaCy's -PRON-  Otherwise, I wil have -PRON- all over the headline\n",
    "    if token.lemma_ == '-PRON-': # if the lemma is -PRON- I just return the lower case of the personal pronoun.\n",
    "        return token.lower_ \n",
    "    else:\n",
    "        return token.is_punct or token.is_space # otherwise, I return TRUE if token is a punctuation or whitespace, and FALSE if otherwise\n",
    "    \n",
    "\n",
    "# This function returns TRUE for alphanumeric characters \n",
    "def get_alphanumeric(token):\n",
    "    return token.is_alpha or token.is_digit\n",
    "\n",
    "# A generator function that reads headlines from \"headline_text_all.txt\" (the text file that contains all headline text)\n",
    "def get_line_headline(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        for headline in f:\n",
    "            yield headline\n",
    "            \n",
    "# This function lemmatizes/normalizes a token (in lowercase)            \n",
    "def lemm(token):\n",
    "    return token.lemma_.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_hl_filepath = os.path.join(scratch_directory, 'lemm_headline_txt_all.txt') # text file that contains cleaned and lemmatized text\n",
    "lemm_hl_sent_filepath = os.path.join(scratch_directory, 'lemm_headline_sentence_all.txt') # contains cleaned, lemmatized, and sentencized text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "execute = False\n",
    "\n",
    "if execute:\n",
    "    \n",
    "    with open(lemm_hl_filepath, 'w') as lemm_hl_file:   # open lemm_headline_txt_all.txt in write mode\n",
    "         with open(lemm_hl_sent_filepath, 'w') as lemm_sentence_file: # open lemm_headline_sentence_all.txt in write mode\n",
    "                \n",
    "                # stream in the headlines from \"headline_txt_all.txt\" 500 at a time.\n",
    "                # nlp.pipe applies nlp to each headline\n",
    "                pipe = nlp.pipe( \n",
    "                    get_line_headline(headline_txt_filepath), # read line for file that contains all headline text (one per line)\n",
    "                    batch_size = 500, \n",
    "                    n_threads=8)\n",
    "        \n",
    "                for headline in pipe:\n",
    "                    # lemmatize each headline (convert from a list of tokens to a string)\n",
    "                    lemm_headline = ' '.join([lemm(token) for token in headline if (not punct_space(token) and \n",
    "                                                                            get_alphanumeric(token))])\n",
    "            \n",
    "                    # Save the lemmatized text in \"lemm_headline_txt_all.txt\"\n",
    "                    lemm_hl_file.write(lemm_headline + '\\n')\n",
    "            \n",
    "            \n",
    "                    # Sentencize each headline, and then lemmatize each sentence\n",
    "                    for sentence in headline.sents: # sentencize each headline\n",
    "                        #lemm each sentence\n",
    "                        lemm_sentence = ' '.join([lemm(token) for token in sentence if (not punct_space(token) and\n",
    "                                                                               get_alphanumeric(token))])\n",
    "                \n",
    "                        #Save each of the lemmatized sentence as a new line in \"lemm_headline_sentence_all.txt\"\n",
    "                        lemm_sentence_file.write(lemm_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In \"lemm_headline_sentence_all.txt\", I have one sentence of each headline per line.\n",
    "- I use gensim LineSentence class to get an iterator of sentences. The LineSentence class allows me to easily work with other gensim components by streaming the docs (sentences) from disk (and not hold entire dataset/corpora in RAM at once).  \n",
    "- Thus, using LineSentence from gensim, I can to up-scale my modelling pipeline to very large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_unigrams = LineSentence(lemm_hl_sent_filepath) # a LineStence object that holds all sentences in my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.LineSentence"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences_unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's Take a look at a few sample sentences in the transformed corpora.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emi', 'have', 'apparently', 'negotiate', 'end', 'well', 'so', 'that', 'child', 'in', 'school', 'will', 'now', 'be', 'indoctrinate', 'about', 'the', 'illegality', 'of', 'download', 'music']\n",
      "['the', 'ignorance', 'and', 'audacity', 'of', 'this', 'get', 'to', 'a', 'little', 'so', 'write', 'an', 'open', 'letter', 'to', 'the', 'dfes', 'about']\n",
      "['unfortunately', 'pedantic', 'as', 'suppose', 'have', 'to', 'be', 'when', 'write', 'to', 'goverment', 'representative']\n",
      "['but', 'hope', 'find', 'useful', 'and', 'perhaps', 'feel', 'inspired', 'to', 'do', 'something', 'similar', 'if', 'or', 'when', 'the', 'same', 'thing', 'have', 'happen', 'in', 'area']\n",
      "['jaschan', 'self', 'confess', 'author', 'of', 'the', 'netsky', 'and', 'sasser', 'virus', 'for', '70', 'percent', 'of', 'virus', 'infection', 'in', '2004', 'accord', 'to', 'a', 'six', 'roundup', 'publish', 'wednesday', 'by', 'antivirus', 'company', 'sophos']\n"
     ]
    }
   ],
   "source": [
    "for sent_unigram in it.islice(sentences_unigrams, 10, 15):  # look at sentences with index 10 to 20   (it.islice(iterator, start, end, step))\n",
    "    print(sent_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emi have apparently negotiate end well so that child in school will now be indoctrinate about the illegality of download music\n",
      "\n",
      "the ignorance and audacity of this get to a little so write an open letter to the dfes about\n",
      "\n",
      "unfortunately pedantic as suppose have to be when write to goverment representative\n",
      "\n",
      "but hope find useful and perhaps feel inspired to do something similar if or when the same thing have happen in area\n",
      "\n",
      "jaschan self confess author of the netsky and sasser virus for 70 percent of virus infection in 2004 accord to a six roundup publish wednesday by antivirus company sophos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_unigram in it.islice(sentences_unigrams, 10, 15): \n",
    "    print(' '.join(sent_unigram)) # convert to string\n",
    "    print('') # add space between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modelling\n",
    "- Learn combinations of tokens that together represents meaningful multi-word concepts (ex: \"United States\", \"happy hour\")\n",
    "- These phrase models are developed by examining all the words in The headlines and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. \n",
    "\n",
    "- Two important parameters for phrase models:\n",
    "    - $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times.\n",
    "    - $threshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase.\n",
    "\n",
    "- Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "- We would get multi-word expressions that represent common concepts, but aren't specifically named entities (ex: happy hour, in addition to named entities such as New York) to become phrases in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Bigrams (first order phrase model)**\n",
    "- Train a phrase model using the headline corpora to identify/link words into 2-word phrases (or bigrams).  For example, words that often appear together in the corpora will be linked together into a single token (ex: happy hour==> happy_hour)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(scratch_directory, 'bigram_phrase_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the bigram model\n",
    "execute = False\n",
    "if execute:\n",
    "    # train the phrase model using the processed sentences (LineSentence object)\n",
    "    bigram_phrases_model = Phrases(sentences_unigrams)\n",
    "    # Use the Phraser function to turn the phrase model into a \"Phraser\" object, which is optimized for speed and memory use\n",
    "    bigram_phrases_model = Phraser(bigram_phrases_model)\n",
    "    #Save the model for future use\n",
    "    bigram_phrases_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from disk\n",
    "bigram_phrases_model = Phraser.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.phrases.Phraser at 0x1ad56de6908>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_phrases_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the trained Phrase model to our lemmatized sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new file that contains transformed sentences (with bigrams)\n",
    "sentences_bigrams_filepath = os.path.join(scratch_directory, 'sentence_bigram_phrases_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute = False\n",
    "\n",
    "if execute:\n",
    "    # write the sentences (with bigrams) in \"sentence_bigram_phrases_all.txt\"\n",
    "    with open(sentences_bigrams_filepath, 'w') as bigram_file:\n",
    "        for sent in sentences_unigrams: # sent is a list of tokens\n",
    "            sent_with_bigram = ' '.join(bigram_phrases_model[sent])  # apply the phrase model to each lemmatized sentences\n",
    "            bigram_file.write(sent_with_bigram + '\\n') # write each transformed sentences into a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see two-word phrases, such as \"plan_to\" linked together in the text as a single token.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first-order transformed data is now organized with one document (sentence) per line in \"sentence_bigram_phrases_all.txt\".  As before, I use the gensim LineSentence class to get an LineSentence object for the entire set of documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_bigrams = LineSentence(sentences_bigrams_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.LineSentence"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Trigrams (second order phrase model)**\n",
    "- I apply the second-order phrase model on top of the transformed data in \"sentence_bigrams_phrases_all.txt\"), so that incomplete word combinations like \"big bad_wolf\" will become fully joined to \"big_bad_wolf\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train trigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new filepath for transformed sentences (with trigrams)\n",
    "trigram_model_filepath = os.path.join(scratch_directory, 'trigram_phrase_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute = False\n",
    "if execute:\n",
    "    trigram_phrases_model = Phrases(sentences_bigrams)\n",
    "    trigram_phrases_model = Phraser(trigram_phrases_model)\n",
    "    \n",
    "    # Save model\n",
    "    trigram_phrases_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from disk\n",
    "trigram_phrases_model = Phraser.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply trained trigram phrase model to our \"first-order transformed sentences\" (aka sentences with bigrams). Then we will write the \"second-order transformed sentences\" (aka sentences with trigrams) into a file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_trigrams_filepath = os.path.join(scratch_directory, 'sentence_trigram_phrases_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute = True\n",
    "if execute:\n",
    "    with open(sentences_trigrams_filepath, 'w') as trigram_file:\n",
    "        for sent_bigram in sentences_bigrams: # iterate the LineSentence object that contains the set of docs with bigrams\n",
    "            sent_trigram = ' '.join(trigram_phrases_model[sent_bigram])\n",
    "            \n",
    "            # write the transformed sentence into 'sentence_trigram_phrases_all.txt'\n",
    "            trigram_file.write(sent_trigram + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The second-order transformed data is now organized with one document (sentence) per line in \"sentence_trigram_phrases_all.txt\". As before, I use gensim LineSentence class to get a LineSentence object for the entire set of documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_trigrams = LineSentence(sentences_trigrams_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process headlines\n",
    "- Now that I have the trained bigram and trigram phraser models (from each individual sentences in my set of headlines), I will now process the lemmatized headlines data (that is not broken down into individual sentences).\n",
    "- I will first create the gensim LineSentence object for the lemmatized headlines corpus.\n",
    "- Second, I will apply the first and second order transformation to the headlines.\n",
    "- Third, I will remove stopwords. Stopwords are very common words, like a, the, and, and so on, that serve functional roles in natural language, but typically don't contribute to the overall meaning of text (adds only noise). Filtering stopwords is a common procedure that allows higher-level NLP modeling techniques to focus on the words that carry more semantic weight.\n",
    "- Finally, I will write the transformed headlines (with trigrams, one for each line) into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_trigrams_filepath = os.path.join(scratch_directory, 'headlines_trigrams_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute = True\n",
    "if execute:\n",
    "    # create the LineSentence iterator for the lemmatized headlines\n",
    "    lemm_headlines = LineSentence(lemm_hl_filepath)\n",
    "    \n",
    "    # write the transformed headlines into \"headlines_trigrams_all.txt\"\n",
    "    with open(headlines_trigrams_filepath, 'w') as headlines_trigram_file:\n",
    "        \n",
    "        for hl_unigram in lemm_headlines: # get each headline from the lemm_headlines LineSentence object\n",
    "            # apply the trained bigram and trigram phrase models to each headline\n",
    "            hl_bigram = bigram_phrases_model[hl_unigram]\n",
    "            hl_trigram = trigram_phrases_model[hl_bigram]\n",
    "            \n",
    "            # remove stopwords from each of the transformed headline\n",
    "            hl_trigram = [t for t in hl_trigram if t not in nlp.Defaults.stop_words]\n",
    "            \n",
    "            # convert the list of tokens into a single string for each transformed headline\n",
    "            hl_trigram = ' '.join(hl_trigram)\n",
    "            \n",
    "            # write the transformed headline (with trigram) into \"headlines_trigrams_all.txt\"\n",
    "            headlines_trigram_file.write(hl_trigram + '\\n')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compared the original headline and transformed headline**\n",
    "- What I did?\n",
    "    - Tokenized and lemmatized text.\n",
    "    - Removed stopwords, punctuations, and non-alphanumeric characters.\n",
    "    - Include bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "Single mothers, poverty were big factors in school performance    HealthDayNews -- American teenagers who live with poor single mothers are more likely to get into trouble at school and have poor marks and are less likely to think they'll go to college, says a Rice University study.    Holly Heard, an assistant professor of sociology, analyzed data from thousands of teens who took part in the National Longitudinal Study of Adolescent Health...\n",
      "\n",
      "\n",
      "Transformed:\n",
      "\n",
      "single mother poverty big factor school performance healthdaynew american teenager live poor single mother likely trouble school poor mark likely think college rice university study holly heard assistant professor sociology analyze datum thousand_of teen take_part national longitudinal study adolescent health\n",
      "\n"
     ]
    }
   ],
   "source": [
    "headline_num = 200\n",
    "\n",
    "with open(\"news_article_data.json\") as file:  \n",
    "    data=[json.loads(line) for line in file]\n",
    "    \n",
    "sample_headline = data[headline_num]['content']\n",
    "print('Original:' + '\\n')\n",
    "print(sample_headline)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('Transformed:' + '\\n')\n",
    "\n",
    "with open(headlines_trigrams_filepath) as file:\n",
    "    for transformed_sample in it.islice(file, headline_num, headline_num+1):\n",
    "        print(transformed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "- Topic modeling is a method for unsupervised classification of documents, similar to clustering on numeric data, which finds some natural groups of items (topics) even when we’re not sure what we’re looking for.\n",
    "- It is similar to clustering for numerical data.\n",
    "- Topic modeling provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives.\n",
    "    - Discovering the hidden themes in the collection.\n",
    "    - Classifying the documents into the discovered themes.\n",
    "    - Using the classification to organize/summarize/search the documents.\n",
    "    - For example, let’s say a document belongs to the topics food, dogs and health. So if a user queries “dog food”, they might find the above-mentioned document relevant because it covers those topics(among other topics).  Therefore, by annotating the document, based on the topics predicted by the modeling method, we are able to optimize our search process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA basics:\n",
    "https://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "- One of the most popular topic modeling methods. \n",
    "-  Each document is made up of various words, and each topic also has various words belonging to it. \n",
    "-  The aim of LDA is to find topics a document belongs to, based on the words in it.\n",
    "- Assumptions:\n",
    "    - We know beforehand how many topics we want. ‘k’ is pre-decided.\n",
    "    - Each document is just a collection of words or a “bag of words”.\n",
    "   \n",
    "- Two parts in LDA:\n",
    "    - The words that belong to a document, that we already know.\n",
    "    - The words that belong to a topic or the probability of words belonging into a topic, that we need to calculate.\n",
    "- We use an algorithm to find the latter.\n",
    "\n",
    "- LDA represents documents as a mixture of topics. Similarly, a topic is a mixture of words. If a word has high probability of being in a topic, all the documents having w will be more strongly associated with t as well. \n",
    "- Similarly, if w is not very probable to be in t, the documents which contain the w will be having very low probability of being in t, because rest of the words in d will belong to some other topic and hence d will have a higher probability for those topic. \n",
    "- So even if w gets added to t, it won’t be bringing many such documents to t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step to creating an LDA model is to learn the full vocabulary of the corpus to be modeled. I use \"gensim.corpora Dictionary\" class for this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict_filepath = os.path.join(scratch_directory, 'trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LineSentence object\n",
    "headline_trigrams = LineSentence(headlines_trigrams_filepath)\n",
    "\n",
    "execute = False\n",
    "if execute:\n",
    "    # learn the dictionary by iterating over all of the headlines\n",
    "    dictionary_trigrams = Dictionary(headline_trigrams)\n",
    "\n",
    "    # Filter tokens that are very rare or too common in the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    dictionary_trigrams.filter_extremes(no_below=20, no_above=0.4)\n",
    "    dictionary_trigrams.compactify()\n",
    "\n",
    "    # Save the corpus dictionary\n",
    "    dictionary_trigrams.save(corpus_dict_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the finished dictionary from disk\n",
    "dictionary_trigrams = Dictionary.load(corpus_dict_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LDA assumes that documents are represented by the bag-of-words model. In the bag-of-words model, a document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, is discarded.\n",
    "- BOW vectors are sparsed (mostly 0s) and large.\n",
    "- I generate a bag-of-words representation for each headline (implemented by the trigram_bow_generator function) Then I save the resulting bag-of-words representations of the headlines as a matrix (each row is a document, each column is a sparse vector for each token in the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus_filepath = os.path.join(scratch_directory, 'bow_trigrams_corpus_all.mm') # file for the BOW matrix (represents documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator function to read headlines from a file and yield a bag-of-words representation\n",
    "def bow_generator(filepath):\n",
    "    for hl in LineSentence(filepath):\n",
    "        yield dictionary_trigrams.doc2bow(hl) # for each document, get its bow representation using the corpora dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute = False\n",
    "if execute:\n",
    "    # Generate bag-of-words representations for all headlines and save them as a matrix\n",
    "    MmCorpus.serialize(bow_corpus_filepath, bow_generator(headlines_trigrams_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(bow_corpus_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With the bag-of-words corpus, we are start to learn our topic model from the headlines. We need to pass the bag-of-words matrix and Dictionary from our to the LdaMulticore function as inputs. \n",
    "- We also need to specify the number of topics the model should learn. \n",
    "- We specifiy K=4 topics (which coincide with the \"true\" number of topics that we have in our headline corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business', 'SciTech', 'Sports', 'World'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(label) # I have 4 \"true\" classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join(scratch_directory, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    " with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(\n",
    "            trigram_bow_corpus,\n",
    "            num_topics=4,\n",
    "            id2word=dictionary_trigrams,\n",
    "            workers=1\n",
    "            )\n",
    "lda.save(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our topic model is now trained and ready to be used. \n",
    "- Since each topic is represented as a mixture of tokens, you can manually inspect which tokens have been grouped together into which topics to try to understand the patterns the model has discovered in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a user-supplied topic number and print out a formatted list of the top tokens\n",
    "def explore_topic(topic_number, topn=15):\n",
    "    print(f'{\"term\":20} {\"frequency\"}' + '\\n') # column headers\n",
    "    \n",
    "    for term, frequency in lda.show_topic(topic_number, topn):\n",
    "        print(f'{term:20} {round(frequency, 3):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business', 'SciTech', 'Sports', 'World'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "ap                   0.015\n",
      "team                 0.009\n",
      "company              0.008\n",
      "new                  0.006\n",
      "end                  0.005\n",
      "report               0.005\n",
      "million              0.005\n",
      "today                0.005\n",
      "afp                  0.005\n",
      "government           0.005\n",
      "start                0.005\n",
      "day                  0.005\n",
      "on_tuesday           0.005\n",
      "yesterday            0.004\n",
      "microsoft            0.004\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=0)   # Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "new                  0.015\n",
      "reuters              0.006\n",
      "lead                 0.006\n",
      "yesterday            0.006\n",
      "quot                 0.005\n",
      "company              0.005\n",
      "ap                   0.005\n",
      "internet             0.005\n",
      "time                 0.005\n",
      "second               0.005\n",
      "today                0.005\n",
      "release              0.005\n",
      "tuesday              0.005\n",
      "country              0.004\n",
      "base                 0.004\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=1)    # World or Business?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "company              0.014\n",
      "win                  0.010\n",
      "ap                   0.007\n",
      "yesterday            0.007\n",
      "year                 0.006\n",
      "reuters              0.006\n",
      "service              0.006\n",
      "game                 0.005\n",
      "announce             0.005\n",
      "new                  0.005\n",
      "million              0.004\n",
      "report               0.004\n",
      "on_wednesday         0.004\n",
      "more_than            0.004\n",
      "on_monday            0.004\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=2) # SciTech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "reuters              0.007\n",
      "new                  0.007\n",
      "company              0.007\n",
      "report               0.006\n",
      "government           0.006\n",
      "group                0.006\n",
      "on_friday            0.005\n",
      "big                  0.005\n",
      "on_wednesday         0.004\n",
      "quot                 0.004\n",
      "country              0.004\n",
      "record               0.004\n",
      "ap                   0.004\n",
      "leader               0.004\n",
      "year                 0.004\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=3) # sports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In case the news headlines are not labelled in this problem, then I can apply LDA to the corpus and examine the tokens that are grouped in each topic and figure out what the topic might be. Then I can labelled each of the documents/headlines with these topics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {0: \"Business\", 1: \"World\", 2: \"SciTech\", 3:\"Sports\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Manually review the top terms for each topic is a helpful exercise to figure out what the topic might be. \n",
    "- But to get a deeper understanding of the topics and how they relate to each other, we need to visualize the data — preferably in an interactive format.  - We use the pyLDAvis libary for that.  It is a library that takes topic models from gensim and prepare visualizations with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join(scratch_directory, 'ldavis_prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The pyLDAvis.gensim.prepare function takes the gensim lda model, the bow representation of the documents (a matrix), and \n",
    "the corpus dictionary generated from gensim as inputs\n",
    "'''\n",
    "execute = False\n",
    "if execute:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda, trigram_bow_corpus, dictionary_trigrams)\n",
    "\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:  # write binary (file mode)\n",
    "        pickle.dump(LDAvis_prepared, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the pyLDAvis display function to display the topic model visualization inline in the notebook\n",
    "\n",
    "- The interactive visualization pyLDAvis produces is helpful for both:\n",
    "    - (1) Better understanding and interpreting individual topics, and\n",
    "    - (2) Better understanding the relationships between the topics.\n",
    "\n",
    "- For (1), you can manually select each topic to view its top most freqeuent and/or \"relevant\" terms, using different values of the $\\lambda$ parameter. This can help when you're trying to assign a human interpretable name or \"meaning\" to each topic.\n",
    "\n",
    "- For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1549618441691292885512478392\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1549618441691292885512478392_data = {\"mdsDat\": {\"x\": [0.03621387266230651, -0.0719012457281018, -0.004080690876772544, 0.03976806394256778], \"y\": [-0.0363298133806536, 0.013989913700627193, -0.03268327470813684, 0.05502317438816323], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [30.07128143310547, 27.253379821777344, 23.246129989624023, 19.429203033447266]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"Freq\": [599.0, 180.0, 145.0, 238.0, 179.0, 291.0, 116.0, 160.0, 231.0, 225.0, 182.0, 182.0, 534.0, 165.0, 262.0, 127.0, 74.0, 122.0, 113.0, 65.0, 78.0, 75.0, 72.0, 67.0, 78.0, 141.0, 101.0, 58.0, 43.0, 46.0, 18.33891487121582, 24.06517791748047, 23.351638793945312, 21.759435653686523, 27.06173324584961, 16.4117374420166, 62.051536560058594, 28.679399490356445, 35.43497848510742, 40.96955108642578, 21.992685317993164, 37.675479888916016, 33.48448944091797, 19.18183708190918, 53.422340393066406, 63.8026008605957, 36.210262298583984, 35.5391731262207, 38.81175231933594, 43.03596496582031, 25.37150764465332, 25.696365356445312, 86.915283203125, 41.01902770996094, 16.93114471435547, 42.6226806640625, 28.134309768676758, 26.43215560913086, 11.843012809753418, 16.45829200744629, 66.67648315429688, 91.78389739990234, 36.697994232177734, 75.42430877685547, 72.06092834472656, 63.1201286315918, 304.20281982421875, 113.03411102294922, 42.34124755859375, 96.74152374267578, 160.72023010253906, 151.4318084716797, 91.40354919433594, 114.12935638427734, 73.31037139892578, 200.34429931640625, 101.1703109741211, 73.39859008789062, 90.01611328125, 71.46049499511719, 63.41114044189453, 89.1324462890625, 73.29002380371094, 87.68428039550781, 76.89137268066406, 89.51591491699219, 81.4564208984375, 93.06365203857422, 68.74213409423828, 84.57735443115234, 89.07110595703125, 77.5320053100586, 95.84180450439453, 91.03147888183594, 78.81207275390625, 75.43389892578125, 27.733436584472656, 13.610099792480469, 39.94908142089844, 53.81303405761719, 16.98116683959961, 29.298402786254883, 18.874540328979492, 24.540910720825195, 17.52811622619629, 30.649202346801758, 111.5901870727539, 138.6674346923828, 20.960662841796875, 28.791637420654297, 18.141136169433594, 54.41236877441406, 23.915096282958984, 21.757171630859375, 25.29413604736328, 12.811117172241211, 21.92181968688965, 39.796024322509766, 22.500408172607422, 33.76763916015625, 17.386327743530273, 10.832681655883789, 26.34810447692871, 78.90653228759766, 21.16505241394043, 69.96149444580078, 121.56301879882812, 27.42220687866211, 172.9796905517578, 71.61494445800781, 33.73728942871094, 46.36199188232422, 92.3878173828125, 117.34162902832031, 82.5727310180664, 133.50982666015625, 157.1389923095703, 222.07479858398438, 136.19557189941406, 61.325958251953125, 84.52998352050781, 62.90048599243164, 55.95542526245117, 108.27700805664062, 152.17994689941406, 73.87549591064453, 53.54804611206055, 124.86444854736328, 73.9670639038086, 80.6622543334961, 79.37648010253906, 81.80965423583984, 117.47997283935547, 62.931182861328125, 67.01175689697266, 65.75407409667969, 64.23360443115234, 63.39383316040039, 26.779579162597656, 15.528257369995117, 18.798959732055664, 21.409645080566406, 42.88163757324219, 19.64909553527832, 25.895069122314453, 20.04532241821289, 14.483794212341309, 32.599395751953125, 13.004776000976562, 12.263240814208984, 20.0020751953125, 16.785377502441406, 11.765085220336914, 22.45155143737793, 26.859996795654297, 12.894852638244629, 34.622745513916016, 9.256793022155762, 14.218730926513672, 23.827041625976562, 30.440780639648438, 23.528972625732422, 17.871187210083008, 15.515430450439453, 16.558795928955078, 28.365461349487305, 30.689435958862305, 13.173422813415527, 22.740324020385742, 132.03085327148438, 49.0109977722168, 49.36685562133789, 117.44878387451172, 39.71997833251953, 50.89678192138672, 29.775772094726562, 58.79851531982422, 67.88397216796875, 34.71942138671875, 38.352943420410156, 33.23601150512695, 98.66730499267578, 60.917850494384766, 159.4087677001953, 73.05278015136719, 95.46967315673828, 66.2774887084961, 62.81364059448242, 108.13517761230469, 156.88584899902344, 52.99552536010742, 73.28826904296875, 70.2720718383789, 57.515663146972656, 64.80606842041016, 134.6236114501953, 76.67217254638672, 40.59457778930664, 44.78053283691406, 48.22110366821289, 64.60314178466797, 78.34129333496094, 54.814762115478516, 61.40269470214844, 57.95139694213867, 53.02067184448242, 73.96549224853516, 58.87278366088867, 55.54924011230469, 57.83778762817383, 57.94955062866211, 56.27022171020508, 23.672565460205078, 17.746219635009766, 22.74352264404297, 32.784297943115234, 29.420448303222656, 29.913068771362305, 27.070331573486328, 23.762359619140625, 12.111129760742188, 16.347654342651367, 27.56684112548828, 18.516923904418945, 25.349096298217773, 21.559974670410156, 25.2254581451416, 26.068944931030273, 45.13413619995117, 7.297265529632568, 14.472383499145508, 16.919193267822266, 34.58039855957031, 11.092371940612793, 14.99779224395752, 66.92133331298828, 16.50261878967285, 15.475970268249512, 13.167679786682129, 43.69832229614258, 9.320712089538574, 14.937723159790039, 32.10099411010742, 24.744482040405273, 34.8304328918457, 41.870906829833984, 25.835512161254883, 21.253292083740234, 35.51945495605469, 34.89267349243164, 34.497291564941406, 59.492984771728516, 49.700050354003906, 93.5069580078125, 68.98609161376953, 47.592594146728516, 69.62991333007812, 41.984779357910156, 56.738685607910156, 111.00917053222656, 80.56584167480469, 128.68333435058594, 55.999881744384766, 41.74711990356445, 42.49369430541992, 68.45877075195312, 105.97665405273438, 50.291351318359375, 57.27025604248047, 60.11359786987305, 40.850791931152344, 62.99543380737305, 53.05587387084961, 55.55990219116211, 49.78114318847656, 49.58695983886719, 55.95426940917969, 53.544864654541016, 48.74961471557617, 57.440128326416016, 45.279747009277344, 47.04861831665039, 45.27454376220703], \"Term\": [\"company\", \"season\", \"score\", \"today\", \"run\", \"game\", \"file\", \"tuesday\", \"service\", \"time\", \"software\", \"billion\", \"ap\", \"security\", \"end\", \"industry\", \"oracle\", \"japan\", \"saturday\", \"yard\", \"talk\", \"intel\", \"throw\", \"consider\", \"death\", \"accord_to\", \"title\", \"site\", \"sprint\", \"word\", \"tokyo_reuters\", \"technologies\", \"general_motors\", \"next_generation\", \"moscow\", \"flat\", \"intel\", \"processor\", \"device\", \"chip\", \"corp\", \"tokyo\", \"produce\", \"darfur_region\", \"pakistan\", \"cut\", \"mobile_phone\", \"madrid\", \"chinese\", \"chairman\", \"format\", \"sanction\", \"japan\", \"sony\", \"world_big\", \"acquire\", \"three_year\", \"regulator\", \"investigation\", \"dell\", \"search\", \"price\", \"car\", \"deal\", \"computer\", \"maker\", \"company\", \"on_wednesday\", \"server\", \"billion\", \"yesterday\", \"report\", \"plan_to\", \"million\", \"offer\", \"new\", \"use\", \"work\", \"announce\", \"percent\", \"google\", \"on_thursday\", \"base\", \"afp\", \"business\", \"on_monday\", \"on_friday\", \"on_tuesday\", \"stock\", \"group\", \"win\", \"world\", \"ap\", \"reuters\", \"year\", \"government\", \"parent\", \"seventh\", \"ban\", \"yard\", \"democratic\", \"philadelphia\", \"minnesota\", \"60\", \"touchdown\", \"lawsuit\", \"score\", \"season\", \"vioxx\", \"candidate\", \"prisoner\", \"throw\", \"olympic\", \"course\", \"second_half\", \"cisco_systems\", \"soccer\", \"m\", \"eastern\", \"16\", \"cover\", \"playoff\", \"williams\", \"saturday\", \"debut\", \"title\", \"run\", \"florida\", \"game\", \"news\", \"holiday\", \"fight\", \"39\", \"country\", \"thursday\", \"team\", \"win\", \"ap\", \"year\", \"coach\", \"point\", \"right\", \"man\", \"end\", \"reuters\", \"new_york\", \"victory\", \"yesterday\", \"second\", \"state\", \"launch\", \"quot\", \"new\", \"sunday\", \"day\", \"start\", \"lead\", \"use\", \"content\", \"martin\", \"fallujah\", \"controversial\", \"site\", \"spam\", \"activity\", \"preliminary\", \"indonesia\", \"tax\", \"earn\", \"deny\", \"satellite\", \"surprise\", \"quickly\", \"insurgent\", \"music\", \"speed\", \"researcher\", \"journalist\", \"portable\", \"message\", \"december\", \"link\", \"murder\", \"airport\", \"fee\", \"video_game\", \"digital\", \"diplomat\", \"island\", \"today\", \"baghdad\", \"like\", \"service\", \"britain\", \"charge\", \"great\", \"kill\", \"internet\", \"iraqi\", \"mobile\", \"control\", \"group\", \"china\", \"ap\", \"software\", \"government\", \"player\", \"rise\", \"report\", \"company\", \"major\", \"afp\", \"big\", \"accord_to\", \"home\", \"new\", \"quot\", \"add\", \"free\", \"this_week\", \"microsoft\", \"win\", \"tuesday\", \"launch\", \"market\", \"official\", \"reuters\", \"world\", \"start\", \"lead\", \"game\", \"on_tuesday\", \"exchange\", \"tool\", \"texas\", \"word\", \"sprint\", \"merger\", \"bid\", \"italian\", \"airbus\", \"founder\", \"aim_at\", \"pledge\", \"mean\", \"reform\", \"eu\", \"form\", \"oracle\", \"olympics\", \"michigan\", \"commissioner\", \"trial\", \"newsfactor\", \"promote\", \"file\", \"partner\", \"discover\", \"seem_to\", \"death\", \"quarterly_profit\", \"palestinians\", \"want\", \"stop\", \"peoplesoft\", \"talk\", \"proposal\", \"north\", \"party\", \"chief\", \"consider\", \"industry\", \"rival\", \"time\", \"tuesday\", \"online\", \"security\", \"president_bush\", \"wednesday\", \"reuters\", \"end\", \"new\", \"include\", \"raise\", \"test\", \"million\", \"company\", \"large\", \"billion\", \"on_thursday\", \"trade\", \"government\", \"sale\", \"microsoft\", \"share\", \"business\", \"lead\", \"on_monday\", \"2004\", \"ap\", \"technology\", \"day\", \"software\"], \"Total\": [599.0, 180.0, 145.0, 238.0, 179.0, 291.0, 116.0, 160.0, 231.0, 225.0, 182.0, 182.0, 534.0, 165.0, 262.0, 127.0, 74.0, 122.0, 113.0, 65.0, 78.0, 75.0, 72.0, 67.0, 78.0, 141.0, 101.0, 58.0, 43.0, 46.0, 20.978485107421875, 27.74004554748535, 27.01434898376465, 25.810653686523438, 32.109413146972656, 19.697750091552734, 75.01708221435547, 35.065101623535156, 43.43040466308594, 51.46695327758789, 27.63664436340332, 47.95791244506836, 43.04145431518555, 24.684476852416992, 69.00727081298828, 82.8626937866211, 47.43305969238281, 46.848121643066406, 52.435062408447266, 58.46711730957031, 34.57695388793945, 35.29810333251953, 122.14353942871094, 57.90345764160156, 23.92155647277832, 60.28191375732422, 40.19074630737305, 37.828857421875, 16.956960678100586, 23.64736557006836, 96.16720581054688, 138.03427124023438, 54.850643157958984, 120.11465454101562, 115.3576889038086, 100.38031005859375, 599.5169677734375, 201.51040649414062, 64.16363525390625, 182.00137329101562, 366.98468017578125, 343.1004638671875, 181.47811889648438, 249.5124969482422, 138.17092895507812, 581.1312255859375, 226.41770935058594, 141.88629150390625, 194.201416015625, 140.5948944091797, 116.80786895751953, 202.623291015625, 149.7314910888672, 202.87342834472656, 164.5692138671875, 216.09408569335938, 184.2057342529297, 242.7602996826172, 138.24501037597656, 266.23089599609375, 343.94537353515625, 213.89407348632812, 534.7655029296875, 428.18609619140625, 290.8500671386719, 271.2690734863281, 31.663118362426758, 16.098234176635742, 47.77561950683594, 65.45065307617188, 20.72382926940918, 36.23200225830078, 23.354381561279297, 30.66185760498047, 22.510709762573242, 39.73781967163086, 145.3468780517578, 180.74722290039062, 27.35183334350586, 37.89617156982422, 24.103742599487305, 72.70008087158203, 32.30375671386719, 29.60825538635254, 34.63984680175781, 17.643985748291016, 30.407323837280273, 55.245704650878906, 31.413480758666992, 47.186279296875, 24.381994247436523, 15.303239822387695, 37.664329528808594, 113.10038757324219, 30.558406829833984, 101.12413787841797, 179.13531494140625, 39.96504592895508, 291.81640625, 112.8141098022461, 50.40140914916992, 72.77275848388672, 167.34014892578125, 223.0011749267578, 147.92965698242188, 270.99066162109375, 343.94537353515625, 534.7655029296875, 290.8500671386719, 110.6050796508789, 176.22946166992188, 119.56904602050781, 102.04727935791016, 262.905517578125, 428.18609619140625, 155.27960205078125, 96.86756896972656, 366.98468017578125, 166.80799865722656, 201.98831176757812, 217.15968322753906, 246.73696899414062, 581.1312255859375, 147.665283203125, 194.48199462890625, 183.98870849609375, 241.4631805419922, 226.41770935058594, 33.392181396484375, 19.569198608398438, 24.472848892211914, 28.24860382080078, 58.84151077270508, 27.754459381103516, 36.607601165771484, 28.673540115356445, 20.849388122558594, 47.1169548034668, 19.227182388305664, 18.21233558654785, 29.889188766479492, 25.275636672973633, 17.7757568359375, 35.05058288574219, 42.06983184814453, 20.32794952392578, 55.00031280517578, 14.77042293548584, 22.84885025024414, 38.88521194458008, 49.828651428222656, 38.59914016723633, 29.489748001098633, 25.854049682617188, 27.6877498626709, 48.024776458740234, 52.02741622924805, 22.537288665771484, 39.1423225402832, 238.24868774414062, 85.7750015258789, 87.66654205322266, 231.8037567138672, 74.29401397705078, 98.98190307617188, 53.40650939941406, 119.09344482421875, 147.47348022460938, 65.40106964111328, 74.41854858398438, 62.12016677856445, 266.23089599609375, 140.45889282226562, 534.7655029296875, 182.2896728515625, 271.2690734863281, 162.94015502929688, 153.21836853027344, 343.1004638671875, 599.5169677734375, 121.41231536865234, 202.87342834472656, 191.45550537109375, 141.2334747314453, 172.2637939453125, 581.1312255859375, 246.73696899414062, 83.5049819946289, 100.89421081542969, 117.58450317382812, 212.9659881591797, 343.94537353515625, 160.1201171875, 217.15968322753906, 190.16281127929688, 155.10250854492188, 428.18609619140625, 213.89407348632812, 183.98870849609375, 241.4631805419922, 291.81640625, 242.7602996826172, 31.72002410888672, 24.562292098999023, 31.772480010986328, 46.78709030151367, 43.35088348388672, 45.85289764404297, 41.704994201660156, 37.089141845703125, 19.222158432006836, 26.076818466186523, 44.017242431640625, 30.014055252075195, 41.56272888183594, 35.38038635253906, 41.47160339355469, 42.87019729614258, 74.8718032836914, 12.125687599182129, 24.141761779785156, 28.503822326660156, 58.64082336425781, 18.905296325683594, 25.837894439697266, 116.1390380859375, 28.880348205566406, 27.477645874023438, 23.584976196289062, 78.71749114990234, 16.94097137451172, 27.21234703063965, 58.6129264831543, 45.18262481689453, 64.56046295166016, 78.61286926269531, 47.70087432861328, 38.777828216552734, 69.17794036865234, 68.3855972290039, 67.57882690429688, 127.46961975097656, 104.223876953125, 225.9914093017578, 160.1201171875, 102.28361511230469, 165.32080078125, 93.75416564941406, 149.05947875976562, 428.18609619140625, 262.905517578125, 581.1312255859375, 154.2638397216797, 95.42033386230469, 99.81120300292969, 249.5124969482422, 599.5169677734375, 140.32949829101562, 182.00137329101562, 202.623291015625, 98.25741577148438, 271.2690734863281, 181.7618865966797, 212.9659881591797, 162.0740966796875, 164.5692138671875, 241.4631805419922, 216.09408569335938, 163.18064880371094, 534.7655029296875, 150.2754669189453, 194.48199462890625, 182.2896728515625], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0671000480651855, 1.059499979019165, 1.055899977684021, 1.030900001525879, 1.0305999517440796, 1.0190999507904053, 1.0118000507354736, 1.000599980354309, 0.9980999827384949, 0.9735000133514404, 0.9732000231742859, 0.9603000283241272, 0.9505000114440918, 0.949400007724762, 0.9455999732017517, 0.9401999711990356, 0.9315999746322632, 0.9253000020980835, 0.9006999731063843, 0.8952000141143799, 0.8920000195503235, 0.8841000199317932, 0.861299991607666, 0.8568999767303467, 0.8560000061988831, 0.8550000190734863, 0.8450000286102295, 0.8431000113487244, 0.8427000045776367, 0.8392000198364258, 0.8353999853134155, 0.7935000061988831, 0.7997000217437744, 0.736299991607666, 0.7311000227928162, 0.7376999855041504, 0.5231999754905701, 0.6233999729156494, 0.7858999967575073, 0.569599986076355, 0.375900000333786, 0.38370001316070557, 0.5156999826431274, 0.41940000653266907, 0.567799985408783, 0.13670000433921814, 0.3959999978542328, 0.5425000190734863, 0.4327000081539154, 0.5249000191688538, 0.5906999707221985, 0.38040000200271606, 0.487199991941452, 0.3628000020980835, 0.4406999945640564, 0.32030001282691956, 0.3856000006198883, 0.24279999732971191, 0.5029000043869019, 0.05490000173449516, -0.149399995803833, 0.1868000030517578, -0.5174999833106995, -0.3467999994754791, -0.10409999638795853, -0.07829999923706055, 1.1675000190734863, 1.132099986076355, 1.1210999488830566, 1.104200005531311, 1.1008000373840332, 1.0875999927520752, 1.0870000123977661, 1.077299952507019, 1.0498000383377075, 1.0403000116348267, 1.0356999635696411, 1.034999966621399, 1.0339000225067139, 1.0252000093460083, 1.0157999992370605, 1.010200023651123, 0.9993000030517578, 0.9919000267982483, 0.9855999946594238, 0.9799000024795532, 0.9728000164031982, 0.972000002861023, 0.9663000106811523, 0.965399980545044, 0.9617999792098999, 0.9545000195503235, 0.9427000284194946, 0.9399999976158142, 0.932699978351593, 0.9315999746322632, 0.9122999906539917, 0.92330002784729, 0.7770000100135803, 0.8456000089645386, 0.8985999822616577, 0.8490999937057495, 0.7059999704360962, 0.6578999757766724, 0.7168999910354614, 0.5921000242233276, 0.5166000127792358, 0.4212000072002411, 0.5412999987602234, 0.7102000117301941, 0.5652999877929688, 0.6577000021934509, 0.6991000175476074, 0.41290000081062317, 0.265500009059906, 0.5570999979972839, 0.7071999907493591, 0.22190000116825104, 0.4867999851703644, 0.382099986076355, 0.2935999929904938, 0.19609999656677246, -0.2987000048160553, 0.4471000134944916, 0.2345000058412552, 0.2709999978542328, -0.02419999986886978, 0.027000000700354576, 1.2382999658584595, 1.2276999950408936, 1.1952999830245972, 1.1818000078201294, 1.1426000595092773, 1.113700032234192, 1.1128000020980835, 1.101099967956543, 1.0946999788284302, 1.0907000303268433, 1.0679999589920044, 1.0635000467300415, 1.0573999881744385, 1.0497000217437744, 1.0463000535964966, 1.0135999917984009, 1.0103000402450562, 1.0039000511169434, 0.9962000250816345, 0.9918000102043152, 0.9847000241279602, 0.9692000150680542, 0.9661999940872192, 0.9639999866485596, 0.9581999778747559, 0.9484000205993652, 0.9449999928474426, 0.9325000047683716, 0.9312000274658203, 0.9221000075340271, 0.9160000085830688, 0.8687999844551086, 0.8992999792098999, 0.8848000168800354, 0.7791000008583069, 0.8328999876976013, 0.7939000129699707, 0.8748000264167786, 0.7531999945640564, 0.6832000017166138, 0.8258000016212463, 0.7961999773979187, 0.8335999846458435, 0.46639999747276306, 0.6236000061035156, 0.24869999289512634, 0.5446000099182129, 0.4147000014781952, 0.559499979019165, 0.567300021648407, 0.3043999969959259, 0.11840000003576279, 0.6299999952316284, 0.4408000111579895, 0.45680001378059387, 0.560699999332428, 0.4814000129699707, -0.0035000001080334187, 0.29019999504089355, 0.7378000020980835, 0.6467000246047974, 0.5677000284194946, 0.2662000060081482, -0.020400000736117363, 0.3871000111103058, 0.19589999318122864, 0.27079999446868896, 0.3856000006198883, -0.2969000041484833, 0.1688999980688095, 0.2614000141620636, 0.029999999329447746, -0.1574999988079071, -0.002899999963119626, 1.3458000421524048, 1.3134000301361084, 1.3041000366210938, 1.2826999425888062, 1.2508000135421753, 1.2111999988555908, 1.2062000036239624, 1.1931999921798706, 1.1764999628067017, 1.1713999509811401, 1.1704000234603882, 1.155400037765503, 1.1439000368118286, 1.1431000232696533, 1.1411999464035034, 1.1410000324249268, 1.1323000192642212, 1.1305999755859375, 1.1267000436782837, 1.1167999505996704, 1.110200047492981, 1.1052000522613525, 1.094499945640564, 1.0871000289916992, 1.0787999629974365, 1.0642999410629272, 1.0555000305175781, 1.0498000383377075, 1.0408999919891357, 1.038599967956543, 1.0362999439239502, 1.0362999439239502, 1.021299958229065, 1.0083999633789062, 1.0252000093460083, 1.0370999574661255, 0.9718000292778015, 0.965499997138977, 0.9660000205039978, 0.8763999938964844, 0.8978999853134155, 0.7559000253677368, 0.7964000105857849, 0.8733000159263611, 0.7736999988555908, 0.8349999785423279, 0.6725000143051147, 0.28839999437332153, 0.45570001006126404, 0.13079999387264252, 0.6251000165939331, 0.8116999864578247, 0.784500002861023, 0.3450999855995178, -0.09449999779462814, 0.6122000217437744, 0.4821999967098236, 0.42329999804496765, 0.760699987411499, 0.17829999327659607, 0.40700000524520874, 0.2946999967098236, 0.4580000042915344, 0.43880000710487366, 0.1762000024318695, 0.24320000410079956, 0.4302000105381012, -0.5927000045776367, 0.43880000710487366, 0.219200000166893, 0.24549999833106995], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.076600074768066, -6.804900169372559, -6.835000038146973, -6.905600070953369, -6.6875, -7.187699794769287, -5.857699871063232, -6.629499912261963, -6.418000221252441, -6.272799968719482, -6.894899845123291, -6.356599807739258, -6.474599838256836, -7.031700134277344, -6.007400035858154, -5.829899787902832, -6.396299839019775, -6.414999961853027, -6.326900005340576, -6.223599910736084, -6.751999855041504, -6.739299774169922, -5.520699977874756, -6.271599769592285, -7.156499862670898, -6.23330020904541, -6.64870023727417, -6.711100101470947, -7.513899803161621, -7.184800148010254, -5.785799980163574, -5.46619987487793, -6.382900238037109, -5.662499904632568, -5.708099842071533, -5.84060001373291, -4.26800012588501, -5.257999897003174, -6.2399001121521, -5.413599967956543, -4.906000137329102, -4.9654998779296875, -5.470399856567383, -5.248300075531006, -5.690899848937988, -4.6855998039245605, -5.368800163269043, -5.689700126647949, -5.4857001304626465, -5.7164998054504395, -5.835999965667725, -5.495500087738037, -5.691199779510498, -5.511899948120117, -5.6433000564575195, -5.491199970245361, -5.585599899291992, -5.452400207519531, -5.755300045013428, -5.547999858856201, -5.496200084686279, -5.635000228881836, -5.422999858856201, -5.474400043487549, -5.618599891662598, -5.662399768829346, -6.564599990844727, -7.276400089263916, -6.199699878692627, -5.901700019836426, -7.055200099945068, -6.509699821472168, -6.949399948120117, -6.6869001388549805, -7.023499965667725, -6.464700222015381, -5.172399997711182, -4.9552001953125, -6.844600200653076, -6.527200222015381, -6.989099979400635, -5.890699863433838, -6.712699890136719, -6.807300090789795, -6.656700134277344, -7.336900234222412, -6.799799919128418, -6.203499794006348, -6.77370023727417, -6.367800235748291, -7.031599998474121, -7.504700183868408, -6.615900039672852, -5.519000053405762, -6.83489990234375, -5.6392998695373535, -5.0868000984191895, -6.575900077819824, -4.734099864959717, -5.616000175476074, -6.36870002746582, -6.05079984664917, -5.361299991607666, -5.122200012207031, -5.473599910736084, -4.993100166320801, -4.830100059509277, -4.4842000007629395, -4.973199844360352, -5.771100044250488, -5.450200080871582, -5.745699882507324, -5.86269998550415, -5.202600002288818, -4.862199783325195, -5.58489990234375, -5.906700134277344, -5.059999942779541, -5.583600044250488, -5.497000217437744, -5.5131001472473145, -5.482900142669678, -5.120999813079834, -5.745200157165527, -5.682400226593018, -5.701300144195557, -5.724699974060059, -5.7378997802734375, -6.4405999183654785, -6.985599994659424, -6.794400215148926, -6.664400100708008, -5.969799995422363, -6.750199794769287, -6.4741997718811035, -6.730199813842773, -7.055200099945068, -6.243899822235107, -7.162899971008301, -7.22160005569458, -6.732399940490723, -6.907700061798096, -7.2631001472473145, -6.6168999671936035, -6.437600135803223, -7.17140007019043, -6.183700084686279, -7.502900123596191, -7.073699951171875, -6.557400226593018, -6.312399864196777, -6.570000171661377, -6.84499979019165, -6.986400127410889, -6.921299934387207, -6.382999897003174, -6.304299831390381, -7.150000095367432, -6.604100227355957, -4.845200061798096, -5.83620023727417, -5.82889986038208, -4.962200164794922, -6.04640007019043, -5.798399925231934, -6.334499835968018, -5.654099941253662, -5.51039981842041, -6.1809000968933105, -6.081399917602539, -6.224599838256836, -5.136499881744385, -5.61870002746582, -4.656700134277344, -5.436999797821045, -5.169400215148926, -5.53439998626709, -5.5879998207092285, -5.0447998046875, -4.672699928283691, -5.757999897003174, -5.433800220489502, -5.475800037384033, -5.676199913024902, -5.55679988861084, -4.825699806213379, -5.388700008392334, -6.024600028991699, -5.926400184631348, -5.852399826049805, -5.559999942779541, -5.367099761962891, -5.724299907684326, -5.610799789428711, -5.668600082397461, -5.757500171661377, -5.424600124359131, -5.6528000831604, -5.710899829864502, -5.670599937438965, -5.668600082397461, -5.6981000900268555, -6.384500026702881, -6.672699928283691, -6.424600124359131, -6.058899879455566, -6.167200088500977, -6.150599956512451, -6.250400066375732, -6.380799770355225, -7.054699897766113, -6.754799842834473, -6.2322001457214355, -6.630199909210205, -6.316100120544434, -6.478000164031982, -6.321000099182129, -6.288099765777588, -5.739200115203857, -7.561399936676025, -6.8765997886657715, -6.720399856567383, -6.0055999755859375, -7.142600059509277, -6.841000080108643, -5.345300197601318, -6.745299816131592, -6.809599876403809, -6.971099853515625, -5.771500110626221, -7.3165998458862305, -6.84499979019165, -6.079999923706055, -6.3403000831604, -5.9984002113342285, -5.814300060272217, -6.297100067138672, -6.492300033569336, -5.978799819946289, -5.996600151062012, -6.007999897003174, -5.4629998207092285, -5.642899990081787, -5.010799884796143, -5.315000057220459, -5.686200141906738, -5.305699825286865, -5.811600208282471, -5.51039981842041, -4.839200019836426, -5.159800052642822, -4.691500186920166, -5.523499965667725, -5.817200183868408, -5.799499988555908, -5.3225998878479, -4.8856000900268555, -5.63100004196167, -5.501100063323975, -5.452600002288818, -5.838900089263916, -5.405799865722656, -5.577499866485596, -5.531400203704834, -5.641200065612793, -5.645100116729736, -5.5243000984191895, -5.568299770355225, -5.662199974060059, -5.4980998039245605, -5.736000061035156, -5.697700023651123, -5.736100196838379]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4], \"Freq\": [0.127155601978302, 0.7205484509468079, 0.10596300661563873, 0.063577800989151, 0.20222985744476318, 0.3431779444217682, 0.15320444107055664, 0.30028069019317627, 0.14939630031585693, 0.549778401851654, 0.23903408646583557, 0.05975852161645889, 0.09784142673015594, 0.8153452277183533, 0.06522762030363083, 0.032613810151815414, 0.16993138194084167, 0.10620711743831635, 0.4106675088405609, 0.3115408718585968, 0.7133151292800903, 0.1327097862958908, 0.0663548931479454, 0.0829436182975769, 0.21853384375572205, 0.08195019513368607, 0.7102349996566772, 0.023950666189193726, 0.33530932664871216, 0.49098867177963257, 0.15567933022975922, 0.43376800417900085, 0.12815871834754944, 0.35983026027679443, 0.07886690646409988, 0.27262043952941895, 0.045436739921569824, 0.06815510988235474, 0.6361143589019775, 0.10404659062623978, 0.10404659062623978, 0.15606987476348877, 0.6242794990539551, 0.07735732197761536, 0.1547146439552307, 0.6188585758209229, 0.1547146439552307, 0.4634363651275635, 0.1544787883758545, 0.2626139521598816, 0.11328444629907608, 0.1795179396867752, 0.4151352345943451, 0.29732659459114075, 0.10658877342939377, 0.046633634716272354, 0.22150975465774536, 0.571262001991272, 0.15155930817127228, 0.8372471332550049, 0.1255870759487152, 0.020931178703904152, 0.4875393807888031, 0.13357242941856384, 0.24710901081562042, 0.13357242941856384, 0.11988972127437592, 0.023977942764759064, 0.21580149233341217, 0.6474044919013977, 0.3603970408439636, 0.1984795331954956, 0.3656201958656311, 0.07312403619289398, 0.5329630374908447, 0.07142803072929382, 0.08241695910692215, 0.3131844401359558, 0.24228061735630035, 0.17498044669628143, 0.5384013652801514, 0.04038010537624359, 0.4678882360458374, 0.12152941524982452, 0.10937647521495819, 0.3038235306739807, 0.13193945586681366, 0.7652487754821777, 0.026387888938188553, 0.07916367053985596, 0.6745591163635254, 0.18231326341629028, 0.09115663170814514, 0.054693982005119324, 0.735456109046936, 0.06841452419757843, 0.10262178629636765, 0.08551815152168274, 0.1717485636472702, 0.2828799784183502, 0.5152456760406494, 0.030308570712804794, 0.11698369681835175, 0.13160665333271027, 0.2339673936367035, 0.5118036866188049, 0.20646610856056213, 0.22070513665676117, 0.4342907667160034, 0.1423904150724411, 0.7437770962715149, 0.07628483325242996, 0.0953560397028923, 0.0953560397028923, 0.7966277003288269, 0.019429944455623627, 0.1360096037387848, 0.05828982964158058, 0.05667653679847717, 0.7367949485778809, 0.17002961039543152, 0.06328823417425156, 0.5515117645263672, 0.14465881884098053, 0.23507057130336761, 0.07016602903604507, 0.21049807965755463, 0.14033205807209015, 0.5964112281799316, 0.5070748925209045, 0.05337630584836006, 0.26187750697135925, 0.17680901288986206, 0.6241456270217896, 0.026006069034337997, 0.23405462503433228, 0.11269296705722809, 0.3847358822822571, 0.014797533862292767, 0.08878520131111145, 0.5031161904335022, 0.17968277633190155, 0.80857253074646, 0.19317398965358734, 0.1609783172607422, 0.5312284231185913, 0.09658699482679367, 0.03539998084306717, 0.14159992337226868, 0.7433995604515076, 0.07079996168613434, 0.7960445284843445, 0.07236768305301666, 0.07236768305301666, 0.07236768305301666, 0.30044683814048767, 0.5246608853340149, 0.10762275010347366, 0.06726422160863876, 0.1350974589586258, 0.7430360317230225, 0.03377436473965645, 0.10132309049367905, 0.08202774822711945, 0.6972358226776123, 0.1640554964542389, 0.04101387411355972, 0.7723620533943176, 0.08447709679603577, 0.12068156898021698, 0.024136314168572426, 0.7697145342826843, 0.04051129147410393, 0.1620451658964157, 0.257093220949173, 0.3445048928260803, 0.15939779579639435, 0.24166761338710785, 0.6244034171104431, 0.1332060545682907, 0.17483295500278473, 0.05827765166759491, 0.1397402286529541, 0.1651475429534912, 0.1397402286529541, 0.5589609146118164, 0.09817265719175339, 0.6872085928916931, 0.13089688122272491, 0.06544844061136246, 0.20068775117397308, 0.10034387558698654, 0.6020632386207581, 0.10034387558698654, 0.676608145236969, 0.16915203630924225, 0.04228800907731056, 0.12686403095722198, 0.1447608917951584, 0.8203117251396179, 0.048253629356622696, 0.27453920245170593, 0.6588940620422363, 0.8058870434761047, 0.04605068638920784, 0.04605068638920784, 0.09210137277841568, 0.2883095443248749, 0.03844127058982849, 0.5958396792411804, 0.07688254117965698, 0.17748363316059113, 0.13311272859573364, 0.5768218040466309, 0.08874181658029556, 0.18196609616279602, 0.10917966067790985, 0.14557287096977234, 0.5458983182907104, 0.0520097017288208, 0.260048508644104, 0.6761261224746704, 0.0520097017288208, 0.063666932284832, 0.7321697473526001, 0.0955004021525383, 0.127333864569664, 0.17877145111560822, 0.4107939600944519, 0.10269849002361298, 0.3080954849720001, 0.16879019141197205, 0.12056442350149155, 0.12056442350149155, 0.6028221249580383, 0.1891549676656723, 0.06305165588855743, 0.7566198706626892, 0.12258482724428177, 0.04086161032319069, 0.7763705849647522, 0.08172322064638138, 0.28893643617630005, 0.6139899492263794, 0.10835116356611252, 0.1786382794380188, 0.6321046352386475, 0.06870702654123306, 0.12367264926433563, 0.18081775307655334, 0.18942812085151672, 0.05166221410036087, 0.5768947601318359, 0.8122755289077759, 0.05076722055673599, 0.10153444111347198, 0.1501311957836151, 0.6755903363227844, 0.07506559789180756, 0.07506559789180756, 0.13995736837387085, 0.13995736837387085, 0.11663114279508591, 0.6064819097518921, 0.7230249643325806, 0.14460498094558716, 0.08676299452781677, 0.05784199386835098, 0.1533929407596588, 0.1917411834001541, 0.6135717630386353, 0.16849331557750702, 0.1189364567399025, 0.4460117220878601, 0.26760703325271606, 0.13707248866558075, 0.5928385257720947, 0.19875510036945343, 0.07196305692195892, 0.8513993620872498, 0.03701736405491829, 0.07403472810983658, 0.03701736405491829, 0.539347231388092, 0.05992747098207474, 0.31675946712493896, 0.08561067283153534, 0.27647826075553894, 0.13639594614505768, 0.35020577907562256, 0.23224173486232758, 0.1685187816619873, 0.24341602623462677, 0.5617292523384094, 0.037448618561029434, 0.3192717432975769, 0.1840507686138153, 0.3718576729297638, 0.12770868837833405, 0.01984071545302868, 0.6745843291282654, 0.31745144724845886, 0.33088788390159607, 0.156736359000206, 0.3773282766342163, 0.13351616263389587, 0.19447201490402222, 0.2787432372570038, 0.16206000745296478, 0.36301442980766296, 0.09592607617378235, 0.09592607617378235, 0.6714825630187988, 0.09592607617378235, 0.22750519216060638, 0.07845006883144379, 0.22750519216060638, 0.46285539865493774, 0.11412078142166138, 0.17118117213249207, 0.62766432762146, 0.08559058606624603, 0.8264784216880798, 0.03999089077115059, 0.07998178154230118, 0.06665148586034775, 0.32548224925994873, 0.08815144002437592, 0.46109986305236816, 0.1288367211818695, 0.707673966884613, 0.058972831815481186, 0.17691849172115326, 0.07645134627819061, 0.12232215702533722, 0.5351594686508179, 0.27522486448287964, 0.2810257375240326, 0.07664338499307632, 0.5875992774963379, 0.07664338499307632, 0.16177241504192352, 0.08088620752096176, 0.10784827917814255, 0.6470896601676941, 0.7122766971588135, 0.05730962008237839, 0.05730962008237839, 0.17192886769771576, 0.1354057341814041, 0.2708114683628082, 0.6093258261680603, 0.1931256651878357, 0.23510949313640594, 0.4954093098640442, 0.0839676782488823, 0.27079126238822937, 0.2992956042289734, 0.07126085460186005, 0.3563042879104614, 0.30852872133255005, 0.36378759145736694, 0.2808992862701416, 0.04604906216263771, 0.07549483329057693, 0.7801132798194885, 0.05032988637685776, 0.10065977275371552, 0.2609093487262726, 0.2650507688522339, 0.24020224809646606, 0.23191942274570465, 0.06844116002321243, 0.13688232004642487, 0.5589361786842346, 0.2395440638065338, 0.20725850760936737, 0.07772193849086761, 0.6217755079269409, 0.10362925380468369, 0.1448076367378235, 0.7240381836891174, 0.07240381836891174, 0.07240381836891174, 0.7684406042098999, 0.14941900968551636, 0.02134557254612446, 0.06403671950101852, 0.2388555109500885, 0.21414631605148315, 0.4365290403366089, 0.11530955135822296, 0.6276131272315979, 0.08965902030467987, 0.1195453554391861, 0.1593938022851944, 0.1077931746840477, 0.548765242099762, 0.2155863493680954, 0.12739193439483643, 0.2734498977661133, 0.19982877373695374, 0.30500179529190063, 0.22612202167510986, 0.8176113963127136, 0.1533021330833435, 0.07218005508184433, 0.2165401577949524, 0.12030009180307388, 0.6015004515647888, 0.08723548799753189, 0.021808871999382973, 0.26170647144317627, 0.6542661786079407, 0.12858358025550842, 0.05143343284726143, 0.6172012090682983, 0.18001702427864075, 0.2071099877357483, 0.04142199829220772, 0.12426599115133286, 0.5799079537391663, 0.3239954113960266, 0.11269405484199524, 0.305213063955307, 0.2629527747631073, 0.45689094066619873, 0.18435950577259064, 0.08416412025690079, 0.2725314497947693, 0.08563703298568726, 0.8135518431663513, 0.08563703298568726, 0.4165628254413605, 0.053750041872262955, 0.5106253623962402, 0.026875020936131477, 0.7589643001556396, 0.10541170835494995, 0.06324702501296997, 0.06324702501296997, 0.8408749103546143, 0.031143514439463615, 0.0934305489063263, 0.031143514439463615, 0.10173027217388153, 0.23737062513828278, 0.6103816032409668, 0.06782017648220062, 0.23770003020763397, 0.047540005296468735, 0.6417900919914246, 0.07131000608205795, 0.3441563546657562, 0.2013314664363861, 0.23230554163455963, 0.22198083996772766, 0.13523991405963898, 0.47655969858169556, 0.2897998094558716, 0.1030399352312088, 0.12409795075654984, 0.6382180452346802, 0.18614692986011505, 0.06204897537827492, 0.10579045861959457, 0.05289522930979729, 0.21158091723918915, 0.5818475484848022, 0.8523612022399902, 0.15497475862503052, 0.07736379653215408, 0.2063034623861313, 0.15472759306430817, 0.5415465831756592, 0.5283311009407043, 0.15922307968139648, 0.17369790375232697, 0.13027341663837433, 0.22565720975399017, 0.32236745953559875, 0.3417094945907593, 0.10960493236780167, 0.09286845475435257, 0.7429476380348206, 0.030956152826547623, 0.15478076040744781, 0.2474086433649063, 0.08246955275535583, 0.5772868394851685, 0.43972572684288025, 0.11943168193101883, 0.28772178292274475, 0.1520039588212967, 0.4164852499961853, 0.09255228191614151, 0.24526353180408478, 0.24989114701747894, 0.4392387568950653, 0.13325220346450806, 0.1283169388771057, 0.2961159944534302, 0.3830939531326294, 0.21832235157489777, 0.23068022727966309, 0.16477158665657043, 0.5607650876045227, 0.2332385778427124, 0.1439131647348404, 0.06451279670000076, 0.12709757685661316, 0.2444184273481369, 0.1564277857542038, 0.4692833721637726, 0.2671232521533966, 0.06678081303834915, 0.06678081303834915, 0.6010273098945618, 0.7680349946022034, 0.08694735914468765, 0.028982453048229218, 0.11592981219291687, 0.11024407297372818, 0.07349605113267899, 0.29398420453071594, 0.5512203574180603, 0.09474746137857437, 0.8843095898628235, 0.31163057684898376, 0.10387686640024185, 0.034625619649887085, 0.5886355638504028, 0.20237664878368378, 0.23128759860992432, 0.05782189965248108, 0.5203971266746521, 0.23234033584594727, 0.09293613582849503, 0.13940420746803284, 0.5421274900436401, 0.5049970149993896, 0.1707032173871994, 0.14225268363952637, 0.17781583964824677, 0.08279973268508911, 0.8003973960876465, 0.027599910274147987, 0.08279973268508911, 0.5014378428459167, 0.314087450504303, 0.15428857505321503, 0.02204122394323349, 0.2577633559703827, 0.23321445286273956, 0.4050566852092743, 0.09819556027650833, 0.0653456375002861, 0.7188020348548889, 0.1960369199514389, 0.0653456375002861, 0.1999063491821289, 0.09995317459106445, 0.06663544476032257, 0.633036732673645, 0.17590701580047607, 0.4823257029056549, 0.2950698435306549, 0.05106978118419647, 0.3063611388206482, 0.08753176033496857, 0.6127222776412964, 0.06975071877241135, 0.06975071877241135, 0.6975072026252747, 0.17437680065631866, 0.14932669699192047, 0.28798720240592957, 0.11732812225818634, 0.4479801058769226, 0.6665011644363403, 0.11591324210166931, 0.13764697313308716, 0.07244578003883362, 0.12446200102567673, 0.7467719912528992, 0.08297466486692429, 0.04148733243346214, 0.8270331025123596, 0.02851838245987892, 0.05703676491975784, 0.11407352983951569, 0.7667027115821838, 0.09293366223573685, 0.09293366223573685, 0.02323341555893421, 0.19351422786712646, 0.11610852926969528, 0.11610852926969528, 0.5805426836013794, 0.2515677213668823, 0.18867579102516174, 0.02096397615969181, 0.5450633764266968, 0.2951424717903137, 0.059028491377830505, 0.059028491377830505, 0.5312564373016357, 0.11251278966665268, 0.6750767230987549, 0.22502557933330536, 0.24722683429718018, 0.33233770728111267, 0.3120732307434082, 0.109428271651268, 0.25151872634887695, 0.18863904476165771, 0.12575936317443848, 0.4401577413082123, 0.19784973561763763, 0.08479274064302444, 0.08479274064302444, 0.6218134760856628, 0.6873059868812561, 0.18504391610622406, 0.079304538667202, 0.026434846222400665, 0.44010433554649353, 0.13115692138671875, 0.31477659940719604, 0.11075473576784134, 0.1999988555908203, 0.09090857207775116, 0.6363599896430969, 0.09090857207775116, 0.2125244140625, 0.3549858331680298, 0.17282205820083618, 0.2592330873012543, 0.19235748052597046, 0.5268922448158264, 0.1839941143989563, 0.10036042332649231, 0.19579897820949554, 0.1566391885280609, 0.4111778438091278, 0.23495876789093018, 0.26865243911743164, 0.13432621955871582, 0.1151367649435997, 0.4797365069389343, 0.1228121891617775, 0.6810494065284729, 0.1730535328388214, 0.02791186049580574, 0.37411582469940186, 0.16505110263824463, 0.1705528050661087, 0.29159027338027954, 0.7365834712982178, 0.2549712061882019, 0.033456914126873016, 0.06691382825374603, 0.6691382527351379, 0.23419839143753052, 0.09725873172283173, 0.6984944939613342, 0.0795753225684166, 0.12378383427858353, 0.03440046310424805, 0.7705703973770142, 0.11696157604455948, 0.08256111294031143, 0.6967031955718994, 0.10398555546998978, 0.16637688875198364, 0.04159422218799591, 0.1217169463634491, 0.7690297961235046, 0.055325884371995926, 0.06085847318172455, 0.2817610800266266, 0.443623811006546, 0.221811905503273, 0.05395424738526344, 0.05773697420954704, 0.7217121720314026, 0.14434243738651276, 0.05773697420954704, 0.1270257532596588, 0.18146537244319916, 0.27219805121421814, 0.42341920733451843, 0.1695994883775711, 0.1695994883775711, 0.08479974418878555, 0.5511983633041382, 0.6545763611793518, 0.12468121200799942, 0.04675545543432236, 0.17143666744232178, 0.2890375852584839, 0.10353585332632065, 0.5047373175621033, 0.0992218628525734, 0.062118615955114365, 0.8696606159210205, 0.062118615955114365, 0.4133911728858948, 0.16042044758796692, 0.11723033338785172, 0.30850085616111755, 0.0849740281701088, 0.13595843315124512, 0.7307766079902649, 0.05098441615700722, 0.0657736286520958, 0.7235099077224731, 0.0986604392528534, 0.1315472573041916, 0.32914644479751587, 0.021943097934126854, 0.4004615247249603, 0.2468598484992981, 0.7080751657485962, 0.08635062724351883, 0.17270125448703766, 0.0518103763461113, 0.18015122413635254, 0.7206048965454102, 0.10809073597192764, 0.14758005738258362, 0.049193352460861206, 0.6395136117935181, 0.14758005738258362, 0.2306758016347885, 0.06920274347066879, 0.6689598560333252, 0.16848860681056976, 0.358717679977417, 0.30436649918556213, 0.16848860681056976, 0.2920961081981659, 0.4010133147239685, 0.14357265830039978, 0.16337579488754272, 0.49911385774612427, 0.20253895223140717, 0.20253895223140717, 0.10126947611570358, 0.199191614985466, 0.04426480457186699, 0.199191614985466, 0.5533100366592407, 0.22347839176654816, 0.4266405701637268, 0.1083531603217125, 0.24379460513591766, 0.07912757992744446, 0.07912757992744446, 0.6725844144821167, 0.15825515985488892, 0.31801408529281616, 0.06360282003879547, 0.08904394507408142, 0.5342636704444885, 0.14856648445129395, 0.08489512652158737, 0.700384795665741, 0.08489512652158737, 0.18081803619861603, 0.49448198080062866, 0.18450820446014404, 0.14391639828681946, 0.8651752471923828, 0.07209793478250504, 0.03604896739125252, 0.03604896739125252, 0.3793034255504608, 0.17301559448242188, 0.139743372797966, 0.2994500696659088, 0.21039722859859467, 0.21039722859859467, 0.15028372406959534, 0.42079445719718933, 0.09442133456468582, 0.15736889839172363, 0.03147377818822861, 0.7238969206809998, 0.16158591210842133, 0.24663114547729492, 0.40821707248687744, 0.17859496176242828, 0.6966778039932251, 0.09952539950609207, 0.17416945099830627, 0.024881349876523018, 0.02751028537750244, 0.7427777051925659, 0.16506171226501465, 0.05502057075500488, 0.14871934056282043, 0.5610774755477905, 0.08787960559129715, 0.20279909670352936, 0.17257292568683624, 0.2522219717502594, 0.1592980921268463, 0.41594502329826355, 0.1681102067232132, 0.6922184824943542, 0.059333015233278275, 0.07911068946123123, 0.10073507577180862, 0.20566745102405548, 0.5540429353713989, 0.13851073384284973, 0.7923614382743835, 0.083406463265419, 0.10425808280706406, 0.8580219149589539, 0.04766788333654404, 0.04766788333654404, 0.04766788333654404, 0.08142562210559845, 0.08142562210559845, 0.12213844060897827, 0.7328306436538696, 0.04442330077290535, 0.7996193766593933, 0.13326989114284515, 0.04442330077290535, 0.2442563772201538, 0.1221281886100769, 0.21372433006763458, 0.41727131605148315, 0.22168856859207153, 0.15347670018672943, 0.03410593420267105, 0.5968538522720337, 0.09367967396974564, 0.13115154206752777, 0.3434921205043793, 0.43092650175094604, 0.44607818126678467, 0.27824679017066956, 0.19874770939350128, 0.07508246600627899, 0.18582069873809814, 0.5574620962142944, 0.12388046830892563, 0.13420383632183075, 0.062467753887176514, 0.2706936001777649, 0.5830323696136475, 0.062467753887176514, 0.03656062111258507, 0.7677730321884155, 0.1096818596124649, 0.1096818596124649, 0.11942758411169052, 0.15354974567890167, 0.17061083018779755, 0.5459546446800232, 0.18784448504447937, 0.20797067880630493, 0.2280968725681305, 0.38239768147468567, 0.02655031904578209, 0.6903083324432373, 0.15930192172527313, 0.13275159895420074, 0.25876203179359436, 0.4564678370952606, 0.22678019106388092, 0.055241331458091736, 0.12824049592018127, 0.08549366891384125, 0.06412024796009064, 0.7053227424621582, 0.5144965052604675, 0.1550537347793579, 0.20438902080059052, 0.12686215341091156, 0.36466649174690247, 0.17765803635120392, 0.2758374810218811, 0.18700845539569855, 0.7106561064720154, 0.2508198022842407, 0.04180330038070679, 0.06111474707722664, 0.8250490427017212, 0.045836057513952255, 0.07639343291521072, 0.2716176211833954, 0.46759486198425293, 0.11002232134342194, 0.1512807011604309, 0.43871042132377625, 0.3406136631965637, 0.10354655981063843, 0.11989600956439972], \"Term\": [\"16\", \"16\", \"16\", \"16\", \"2004\", \"2004\", \"2004\", \"2004\", \"39\", \"39\", \"39\", \"39\", \"60\", \"60\", \"60\", \"60\", \"accord_to\", \"accord_to\", \"accord_to\", \"accord_to\", \"acquire\", \"acquire\", \"acquire\", \"acquire\", \"activity\", \"activity\", \"activity\", \"add\", \"add\", \"add\", \"add\", \"afp\", \"afp\", \"afp\", \"afp\", \"aim_at\", \"aim_at\", \"aim_at\", \"aim_at\", \"airbus\", \"airbus\", \"airbus\", \"airbus\", \"airport\", \"airport\", \"airport\", \"airport\", \"announce\", \"announce\", \"announce\", \"announce\", \"ap\", \"ap\", \"ap\", \"ap\", \"baghdad\", \"baghdad\", \"baghdad\", \"baghdad\", \"ban\", \"ban\", \"ban\", \"base\", \"base\", \"base\", \"base\", \"bid\", \"bid\", \"bid\", \"bid\", \"big\", \"big\", \"big\", \"big\", \"billion\", \"billion\", \"billion\", \"billion\", \"britain\", \"britain\", \"britain\", \"britain\", \"business\", \"business\", \"business\", \"business\", \"candidate\", \"candidate\", \"candidate\", \"candidate\", \"car\", \"car\", \"car\", \"car\", \"chairman\", \"chairman\", \"chairman\", \"chairman\", \"charge\", \"charge\", \"charge\", \"charge\", \"chief\", \"chief\", \"chief\", \"chief\", \"china\", \"china\", \"china\", \"china\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"chip\", \"chip\", \"chip\", \"chip\", \"cisco_systems\", \"cisco_systems\", \"cisco_systems\", \"coach\", \"coach\", \"coach\", \"coach\", \"commissioner\", \"commissioner\", \"commissioner\", \"commissioner\", \"company\", \"company\", \"company\", \"company\", \"computer\", \"computer\", \"computer\", \"computer\", \"consider\", \"consider\", \"consider\", \"consider\", \"content\", \"content\", \"control\", \"control\", \"control\", \"control\", \"controversial\", \"controversial\", \"controversial\", \"controversial\", \"corp\", \"corp\", \"corp\", \"corp\", \"country\", \"country\", \"country\", \"country\", \"course\", \"course\", \"course\", \"course\", \"cover\", \"cover\", \"cover\", \"cover\", \"cut\", \"cut\", \"cut\", \"cut\", \"darfur_region\", \"darfur_region\", \"darfur_region\", \"day\", \"day\", \"day\", \"day\", \"deal\", \"deal\", \"deal\", \"deal\", \"death\", \"death\", \"death\", \"death\", \"debut\", \"debut\", \"debut\", \"debut\", \"december\", \"december\", \"december\", \"december\", \"dell\", \"dell\", \"dell\", \"dell\", \"democratic\", \"democratic\", \"democratic\", \"deny\", \"deny\", \"device\", \"device\", \"device\", \"device\", \"digital\", \"digital\", \"digital\", \"digital\", \"diplomat\", \"diplomat\", \"diplomat\", \"diplomat\", \"discover\", \"discover\", \"discover\", \"discover\", \"earn\", \"earn\", \"earn\", \"earn\", \"eastern\", \"eastern\", \"eastern\", \"eastern\", \"end\", \"end\", \"end\", \"end\", \"eu\", \"eu\", \"eu\", \"eu\", \"exchange\", \"exchange\", \"exchange\", \"fallujah\", \"fallujah\", \"fallujah\", \"fallujah\", \"fee\", \"fee\", \"fee\", \"fight\", \"fight\", \"fight\", \"fight\", \"file\", \"file\", \"file\", \"file\", \"flat\", \"flat\", \"flat\", \"florida\", \"florida\", \"florida\", \"florida\", \"form\", \"form\", \"form\", \"form\", \"format\", \"format\", \"format\", \"format\", \"founder\", \"founder\", \"founder\", \"free\", \"free\", \"free\", \"free\", \"game\", \"game\", \"game\", \"game\", \"general_motors\", \"general_motors\", \"general_motors\", \"general_motors\", \"google\", \"google\", \"google\", \"google\", \"government\", \"government\", \"government\", \"government\", \"great\", \"great\", \"great\", \"great\", \"group\", \"group\", \"group\", \"group\", \"holiday\", \"holiday\", \"holiday\", \"home\", \"home\", \"home\", \"home\", \"include\", \"include\", \"include\", \"include\", \"indonesia\", \"indonesia\", \"indonesia\", \"indonesia\", \"industry\", \"industry\", \"industry\", \"industry\", \"insurgent\", \"insurgent\", \"insurgent\", \"insurgent\", \"intel\", \"intel\", \"intel\", \"intel\", \"internet\", \"internet\", \"internet\", \"internet\", \"investigation\", \"investigation\", \"investigation\", \"iraqi\", \"iraqi\", \"iraqi\", \"iraqi\", \"island\", \"island\", \"island\", \"island\", \"italian\", \"italian\", \"italian\", \"italian\", \"japan\", \"japan\", \"japan\", \"japan\", \"journalist\", \"journalist\", \"journalist\", \"kill\", \"kill\", \"kill\", \"kill\", \"large\", \"large\", \"large\", \"large\", \"launch\", \"launch\", \"launch\", \"launch\", \"lawsuit\", \"lawsuit\", \"lawsuit\", \"lawsuit\", \"lead\", \"lead\", \"lead\", \"lead\", \"like\", \"like\", \"like\", \"like\", \"link\", \"link\", \"link\", \"link\", \"m\", \"m\", \"m\", \"m\", \"madrid\", \"madrid\", \"madrid\", \"madrid\", \"major\", \"major\", \"major\", \"major\", \"maker\", \"maker\", \"maker\", \"maker\", \"man\", \"man\", \"man\", \"man\", \"market\", \"market\", \"market\", \"market\", \"martin\", \"martin\", \"mean\", \"mean\", \"mean\", \"mean\", \"merger\", \"merger\", \"merger\", \"merger\", \"message\", \"message\", \"message\", \"message\", \"michigan\", \"michigan\", \"michigan\", \"michigan\", \"microsoft\", \"microsoft\", \"microsoft\", \"microsoft\", \"million\", \"million\", \"million\", \"million\", \"minnesota\", \"minnesota\", \"minnesota\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobile_phone\", \"mobile_phone\", \"mobile_phone\", \"mobile_phone\", \"moscow\", \"moscow\", \"moscow\", \"moscow\", \"murder\", \"murder\", \"murder\", \"murder\", \"music\", \"music\", \"music\", \"music\", \"new\", \"new\", \"new\", \"new\", \"new_york\", \"new_york\", \"new_york\", \"new_york\", \"news\", \"news\", \"news\", \"news\", \"newsfactor\", \"newsfactor\", \"newsfactor\", \"newsfactor\", \"next_generation\", \"next_generation\", \"north\", \"north\", \"north\", \"north\", \"offer\", \"offer\", \"offer\", \"offer\", \"official\", \"official\", \"official\", \"official\", \"olympic\", \"olympic\", \"olympic\", \"olympic\", \"olympics\", \"olympics\", \"olympics\", \"on_friday\", \"on_friday\", \"on_friday\", \"on_friday\", \"on_monday\", \"on_monday\", \"on_monday\", \"on_monday\", \"on_thursday\", \"on_thursday\", \"on_thursday\", \"on_thursday\", \"on_tuesday\", \"on_tuesday\", \"on_tuesday\", \"on_tuesday\", \"on_wednesday\", \"on_wednesday\", \"on_wednesday\", \"on_wednesday\", \"online\", \"online\", \"online\", \"online\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"pakistan\", \"pakistan\", \"pakistan\", \"pakistan\", \"palestinians\", \"palestinians\", \"palestinians\", \"palestinians\", \"parent\", \"parent\", \"partner\", \"partner\", \"partner\", \"partner\", \"party\", \"party\", \"party\", \"party\", \"peoplesoft\", \"peoplesoft\", \"peoplesoft\", \"peoplesoft\", \"percent\", \"percent\", \"percent\", \"percent\", \"philadelphia\", \"philadelphia\", \"philadelphia\", \"philadelphia\", \"plan_to\", \"plan_to\", \"plan_to\", \"plan_to\", \"player\", \"player\", \"player\", \"player\", \"playoff\", \"playoff\", \"playoff\", \"playoff\", \"pledge\", \"pledge\", \"pledge\", \"pledge\", \"point\", \"point\", \"point\", \"point\", \"portable\", \"portable\", \"portable\", \"preliminary\", \"preliminary\", \"preliminary\", \"preliminary\", \"president_bush\", \"president_bush\", \"president_bush\", \"president_bush\", \"price\", \"price\", \"price\", \"price\", \"prisoner\", \"prisoner\", \"prisoner\", \"prisoner\", \"processor\", \"processor\", \"processor\", \"processor\", \"produce\", \"produce\", \"produce\", \"produce\", \"promote\", \"promote\", \"promote\", \"promote\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"quarterly_profit\", \"quarterly_profit\", \"quarterly_profit\", \"quarterly_profit\", \"quickly\", \"quickly\", \"quickly\", \"quot\", \"quot\", \"quot\", \"quot\", \"raise\", \"raise\", \"raise\", \"raise\", \"reform\", \"reform\", \"reform\", \"reform\", \"regulator\", \"regulator\", \"regulator\", \"regulator\", \"report\", \"report\", \"report\", \"report\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"reuters\", \"reuters\", \"reuters\", \"reuters\", \"right\", \"right\", \"right\", \"right\", \"rise\", \"rise\", \"rise\", \"rise\", \"rival\", \"rival\", \"rival\", \"rival\", \"run\", \"run\", \"run\", \"run\", \"sale\", \"sale\", \"sale\", \"sale\", \"sanction\", \"sanction\", \"satellite\", \"satellite\", \"satellite\", \"satellite\", \"saturday\", \"saturday\", \"saturday\", \"saturday\", \"score\", \"score\", \"score\", \"score\", \"search\", \"search\", \"search\", \"search\", \"season\", \"season\", \"season\", \"season\", \"second\", \"second\", \"second\", \"second\", \"second_half\", \"second_half\", \"second_half\", \"second_half\", \"security\", \"security\", \"security\", \"security\", \"seem_to\", \"seem_to\", \"seem_to\", \"seem_to\", \"server\", \"server\", \"server\", \"server\", \"service\", \"service\", \"service\", \"service\", \"seventh\", \"seventh\", \"seventh\", \"share\", \"share\", \"share\", \"share\", \"site\", \"site\", \"site\", \"site\", \"soccer\", \"soccer\", \"soccer\", \"soccer\", \"software\", \"software\", \"software\", \"software\", \"sony\", \"sony\", \"sony\", \"sony\", \"spam\", \"spam\", \"spam\", \"speed\", \"speed\", \"speed\", \"speed\", \"sprint\", \"sprint\", \"sprint\", \"start\", \"start\", \"start\", \"start\", \"state\", \"state\", \"state\", \"state\", \"stock\", \"stock\", \"stock\", \"stock\", \"stop\", \"stop\", \"stop\", \"stop\", \"sunday\", \"sunday\", \"sunday\", \"sunday\", \"surprise\", \"surprise\", \"surprise\", \"surprise\", \"talk\", \"talk\", \"talk\", \"talk\", \"tax\", \"tax\", \"tax\", \"tax\", \"team\", \"team\", \"team\", \"team\", \"technologies\", \"technologies\", \"technologies\", \"technologies\", \"technology\", \"technology\", \"technology\", \"technology\", \"test\", \"test\", \"test\", \"test\", \"texas\", \"texas\", \"texas\", \"texas\", \"this_week\", \"this_week\", \"this_week\", \"this_week\", \"three_year\", \"three_year\", \"three_year\", \"three_year\", \"throw\", \"throw\", \"throw\", \"throw\", \"thursday\", \"thursday\", \"thursday\", \"thursday\", \"time\", \"time\", \"time\", \"time\", \"title\", \"title\", \"title\", \"title\", \"today\", \"today\", \"today\", \"today\", \"tokyo\", \"tokyo\", \"tokyo\", \"tokyo_reuters\", \"tokyo_reuters\", \"tokyo_reuters\", \"tokyo_reuters\", \"tool\", \"tool\", \"tool\", \"tool\", \"touchdown\", \"touchdown\", \"touchdown\", \"touchdown\", \"trade\", \"trade\", \"trade\", \"trade\", \"trial\", \"trial\", \"trial\", \"trial\", \"tuesday\", \"tuesday\", \"tuesday\", \"tuesday\", \"use\", \"use\", \"use\", \"use\", \"victory\", \"victory\", \"victory\", \"victory\", \"video_game\", \"video_game\", \"video_game\", \"video_game\", \"vioxx\", \"vioxx\", \"vioxx\", \"vioxx\", \"want\", \"want\", \"want\", \"want\", \"wednesday\", \"wednesday\", \"wednesday\", \"wednesday\", \"williams\", \"williams\", \"williams\", \"williams\", \"win\", \"win\", \"win\", \"win\", \"word\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world_big\", \"world_big\", \"world_big\", \"yard\", \"yard\", \"yard\", \"yard\", \"year\", \"year\", \"year\", \"year\", \"yesterday\", \"yesterday\", \"yesterday\", \"yesterday\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 4, 3, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1549618441691292885512478392\", ldavis_el1549618441691292885512478392_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1549618441691292885512478392\", ldavis_el1549618441691292885512478392_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1549618441691292885512478392\", ldavis_el1549618441691292885512478392_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Description for LDA\n",
    "- Besides \"clustering\" documents into topics, one of the main uses of LDA is to provide a compact and qunatitative description of text data.\n",
    "- Once LDA model is trained, it can be used to represent documents as a mixture of topics (some with more weights/probability than others).\n",
    "- This mixture can be interpreted as a probability distribution across the topics, so the LDA representation of a paragraph of text might look like 50% Topic A, 20% Topic B, 20% Topic C, and 10% Topic D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_description(headline_text, min_topic_freq=0.05):\n",
    "    \"\"\"\n",
    "    1) accept the original text of a headline \n",
    "    2) parse it with spaCy\n",
    "    3) apply text pre-proccessing steps\n",
    "    4) create a bag-of-words representation of the document\n",
    "    5) create an LDA representation of the document\n",
    "    6) print a sorted list of the top topics in the LDA representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # parse the headline text with spaCy\n",
    "    parsed_headline = nlp(headline_text)\n",
    "    \n",
    "    # lemmatize the text and remove punctuation and whitespace\n",
    "    headline_unigrams = [\n",
    "        lemm(token)\n",
    "        for token in parsed_headline\n",
    "        if (not punct_space(token) and get_alphanumeric(token))\n",
    "        ]\n",
    "    \n",
    "    # apply the first-order and secord-order phrase models\n",
    "    headline_bigrams = bigram_phrases_model[headline_unigrams]\n",
    "    headline_trigrams = trigram_phrases_model[headline_unigrams]\n",
    "    \n",
    "    # remove any remaining stopwords\n",
    "    headline_trigrams = [t for t in headline_trigrams if not t in nlp.Defaults.stop_words]\n",
    "        \n",
    "    # create a bag-of-words representation\n",
    "    headline_bow = dictionary_trigrams.doc2bow(headline_trigrams)\n",
    "    \n",
    "    # create an LDA representation\n",
    "    headline_lda = lda[headline_bow]\n",
    "    \n",
    "    # sort with the most highly related topics first\n",
    "    headline_lda = sorted(headline_lda, key=lambda topic_number_freq: -topic_number_freq[-1])\n",
    "    \n",
    "    for topic_number, freq in headline_lda:\n",
    "        if freq < min_topic_freq:\n",
    "            break\n",
    "            \n",
    "        # print the most highly related topic names and frequencies\n",
    "        print(f'{topic_names[topic_number]:25} {round(freq, 3):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here I use \"existing\" headlines from the dataset.  However, if I have \"new\" headlines that are unlabelled, then I can use LDA (trained from existing labelled headlines) to estimate the label for the new unlabelled headlines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The global extinction crisis is worse than thought, because thousands of  quot;affiliated quot; species also at risk do not figure in calculations.\n"
     ]
    }
   ],
   "source": [
    "headline_num = 1452 \n",
    "\n",
    "with open(\"news_article_data.json\") as file:\n",
    "    # convert the json record to a Python dict. \"data\" is a list of dicts\n",
    "    data=[json.loads(line) for line in file]\n",
    "    \n",
    "\n",
    "sample_headline = data[headline_num]['content']\n",
    "print(sample_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sports                    0.919\n"
     ]
    }
   ],
   "source": [
    "lda_description(sample_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings representations with word2vec\n",
    "- The goal of word vector embedding models is to learn dense, numerical vector representations for each term in a corpus vocabulary.\n",
    "- The vectors it learns about each term should encode some information about the meaning or concept the term represents, and the relationship between it and other terms in the vocabulary. \n",
    "- Word vector models are also fully unsupervised — they learn all of these meanings and relationships solely by analyzing the text of the corpus, without any advance knowledge provided.\n",
    "- Perhaps the best-known word vector model is word2vec, originally proposed in 2013. The general idea of word2vec is, for a given focus word, to use the context of the word — i.e., the other words immediately before and after it — to provide hints about what the focus word might mean. To do this, word2vec uses a sliding window technique, where it considers snippets of text only a few tokens long at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the start of the learning process, the model initializes random vectors for all terms in the corpus vocabulary. \n",
    "- The model then slides the window across every snippet of text in the corpus, with each word taking turns as the focus word. \n",
    "- Each time the model considers a new snippet, it tries to learn some information about the focus word based on the surrouding context, and it \"nudges\" the words' vector representations accordingly. \n",
    "- One complete pass sliding the window across all of the corpus text is known as a training epoch. It's common to train a word2vec model for multiple passes/epochs over the corpus. \n",
    "- Over time, the model rearranges the terms' vector representations such that terms that frequently appear in similar contexts have vector representations that are close to each other in vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2vec has a number of user-defined hyperparameters, including:\n",
    "    - The dimensionality of the vectors. I choose 300 for this example.\n",
    "    - The width of the sliding window, in tokens. Five is a common default choice, but narrower and wider windows are possible.\n",
    "    - The number of training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If I want to train my own word2vec model with my corpus, gensim comes with a highly-optimized implementation word2vec algorithm with its Word2Vec class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences_trigrams = LineSentence(sentences_trigrams_filepath) # get LineSentence iterator of my headlines (with trigrams and stopwords removed)\n",
    "word2vec_filepath = os.path.join(scratch_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute = False\n",
    "\n",
    "if execute:\n",
    "    \n",
    "    # initiate the model and perform the 20 epoch of training\n",
    "    w2v = Word2Vec( \n",
    "        sentences_trigrams, # LineSentence object of the transformed headline corpus\n",
    "        size=300, # size of the word vector\n",
    "        window=5, # size of the window\n",
    "        min_count=3, # Ignores all words with total frequency lower than this.\n",
    "        sg=1, # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "        workers=1, # Use these many worker threads to train the model\n",
    "        iter=20 # number of epoch\n",
    "        )\n",
    "    \n",
    "    # save model\n",
    "    w2v.save(word2vec_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The training is streamed, meaning sentences can be a generator, reading input data from disk on-the-fly, without loading the entire corpus into RAM. It also means you can continue training the model later:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model was trained with 20 epochs.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "w2v = Word2Vec.load(word2vec_filepath)\n",
    "\n",
    "print(f'The model was trained with {w2v.epochs} epochs.')\n",
    "\n",
    "# train with new words if necessary\n",
    "# w2v.train([['hello','world','I','am','here']], total_examples=1, epochs=1)\n",
    "\n",
    "# The trained word vectors are stored in a KeyedVectors instance in w2v.wv:\n",
    "# vector = w2v.wv['computer']  # numpy vector of a word\n",
    "\n",
    "\n",
    "# Gensim instances of KeyedVectors (the common interface of sets of word-vectors) \n",
    "# contain a method init_sims(), which internally calculates unit-length \n",
    "# normalized vectors using a native vector operation for speed.\n",
    "w2v.init_sims() # precompute the normalized vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for separating the trained vectors into KeyedVectors is that if you don’t need the full model state any more (don’t need to continue training), the state can discarded, resulting in a much smaller and faster object that can be mmapped for lightning fast loading and sharing the vectors in RAM between processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# path = get_tmpfile(\"wordvectors.kv\") # path the save model as a keyvector file\n",
    "# word_vectors = KeyedVectors.load(word2vec_filepath, mmap='r')\n",
    "# vector = word_vectors['computer']  # numpy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561 terms in the w2v vocabulary, each with its 300x1 dense vector representations.\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(w2v.wv.vocab):,} terms in the w2v vocabulary, each with its 300x1 dense vector representations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a DataFrame that contains a set of word embeddings that I learnt\n",
    "# I can get the index and count (frequency of word)\n",
    "ordered_vocab=[(term, voc.index, voc.count) for term, voc in w2v.wv.vocab.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('union', 434, 61),\n",
       " ('worker', 294, 81),\n",
       " ('at', 15, 1070),\n",
       " ('say', 13, 1133),\n",
       " ('be', 6, 3754)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda term_tuple: -term_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0, 12464),\n",
       " ('a', 1, 6424),\n",
       " ('of', 2, 5085),\n",
       " ('to', 3, 4820),\n",
       " ('in', 4, 4548)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'a', 'of', 'to', 'in')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_terms[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3, 4)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_indices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12464, 6424, 5085, 4820, 4548)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>injury</th>\n",
       "      <td>-0.028678</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>-0.046433</td>\n",
       "      <td>0.042980</td>\n",
       "      <td>-0.050665</td>\n",
       "      <td>-0.097703</td>\n",
       "      <td>0.064402</td>\n",
       "      <td>-0.070111</td>\n",
       "      <td>-0.047728</td>\n",
       "      <td>-0.100487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>-0.022889</td>\n",
       "      <td>0.005913</td>\n",
       "      <td>-0.019160</td>\n",
       "      <td>0.071120</td>\n",
       "      <td>0.065715</td>\n",
       "      <td>-0.033689</td>\n",
       "      <td>-0.061767</td>\n",
       "      <td>-0.013519</td>\n",
       "      <td>0.056492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>0.015104</td>\n",
       "      <td>0.026804</td>\n",
       "      <td>0.020081</td>\n",
       "      <td>0.060929</td>\n",
       "      <td>0.007962</td>\n",
       "      <td>-0.014428</td>\n",
       "      <td>0.025457</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>0.097565</td>\n",
       "      <td>-0.015763</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062230</td>\n",
       "      <td>-0.069003</td>\n",
       "      <td>-0.033870</td>\n",
       "      <td>-0.006200</td>\n",
       "      <td>0.068917</td>\n",
       "      <td>0.057053</td>\n",
       "      <td>-0.040870</td>\n",
       "      <td>-0.122160</td>\n",
       "      <td>-0.024437</td>\n",
       "      <td>-0.005812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <td>0.044040</td>\n",
       "      <td>-0.012662</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.082535</td>\n",
       "      <td>-0.037324</td>\n",
       "      <td>-0.018668</td>\n",
       "      <td>0.063906</td>\n",
       "      <td>0.016214</td>\n",
       "      <td>-0.075572</td>\n",
       "      <td>-0.100135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085299</td>\n",
       "      <td>-0.071514</td>\n",
       "      <td>-0.059542</td>\n",
       "      <td>-0.056584</td>\n",
       "      <td>-0.014410</td>\n",
       "      <td>0.062958</td>\n",
       "      <td>-0.046979</td>\n",
       "      <td>-0.038288</td>\n",
       "      <td>0.031768</td>\n",
       "      <td>0.111603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receive</th>\n",
       "      <td>-0.047220</td>\n",
       "      <td>-0.005763</td>\n",
       "      <td>0.046993</td>\n",
       "      <td>0.097229</td>\n",
       "      <td>0.050526</td>\n",
       "      <td>-0.048636</td>\n",
       "      <td>-0.066621</td>\n",
       "      <td>-0.022332</td>\n",
       "      <td>-0.029707</td>\n",
       "      <td>-0.148279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025118</td>\n",
       "      <td>-0.040520</td>\n",
       "      <td>-0.031505</td>\n",
       "      <td>-0.143225</td>\n",
       "      <td>-0.014615</td>\n",
       "      <td>0.158997</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>0.008274</td>\n",
       "      <td>0.027896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokyo</th>\n",
       "      <td>-0.054072</td>\n",
       "      <td>-0.102392</td>\n",
       "      <td>0.026237</td>\n",
       "      <td>0.039517</td>\n",
       "      <td>0.032118</td>\n",
       "      <td>-0.006891</td>\n",
       "      <td>-0.020864</td>\n",
       "      <td>0.052435</td>\n",
       "      <td>-0.102598</td>\n",
       "      <td>-0.110021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029326</td>\n",
       "      <td>-0.031980</td>\n",
       "      <td>-0.043148</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>-0.081997</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>-0.051460</td>\n",
       "      <td>0.011985</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.040532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5    \\\n",
       "injury   -0.028678  0.003672 -0.046433  0.042980 -0.050665 -0.097703   \n",
       "question  0.015104  0.026804  0.020081  0.060929  0.007962 -0.014428   \n",
       "airline   0.044040 -0.012662  0.028198  0.082535 -0.037324 -0.018668   \n",
       "receive  -0.047220 -0.005763  0.046993  0.097229  0.050526 -0.048636   \n",
       "tokyo    -0.054072 -0.102392  0.026237  0.039517  0.032118 -0.006891   \n",
       "\n",
       "               6         7         8         9    ...       290       291  \\\n",
       "injury    0.064402 -0.070111 -0.047728 -0.100487  ...  0.014500 -0.022889   \n",
       "question  0.025457  0.004132  0.097565 -0.015763  ... -0.062230 -0.069003   \n",
       "airline   0.063906  0.016214 -0.075572 -0.100135  ... -0.085299 -0.071514   \n",
       "receive  -0.066621 -0.022332 -0.029707 -0.148279  ... -0.025118 -0.040520   \n",
       "tokyo    -0.020864  0.052435 -0.102598 -0.110021  ... -0.029326 -0.031980   \n",
       "\n",
       "               292       293       294       295       296       297  \\\n",
       "injury    0.005913 -0.019160  0.071120  0.065715 -0.033689 -0.061767   \n",
       "question -0.033870 -0.006200  0.068917  0.057053 -0.040870 -0.122160   \n",
       "airline  -0.059542 -0.056584 -0.014410  0.062958 -0.046979 -0.038288   \n",
       "receive  -0.031505 -0.143225 -0.014615  0.158997  0.005598  0.012431   \n",
       "tokyo    -0.043148  0.001361 -0.081997  0.072917 -0.051460  0.011985   \n",
       "\n",
       "               298       299  \n",
       "injury   -0.013519  0.056492  \n",
       "question -0.024437 -0.005812  \n",
       "airline   0.031768  0.111603  \n",
       "receive   0.008274  0.027896  \n",
       "tokyo     0.001258  0.040532  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame with the w2v vectors as data, and the terms as row labels\n",
    "word_vectors_df = pd.DataFrame(\n",
    "    w2v.wv.vectors_norm[term_indices, :],\n",
    "    index=ordered_terms\n",
    "    )\n",
    "\n",
    "word_vectors_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The w2v model has \"embedded\" the terms into a 300-dimensional vector space.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function prints the 10 most similar words to the target word\n",
    "def get_related_terms(token, top_n=10):\n",
    "\n",
    "    for word, similarity in w2v.wv.most_similar(positive=[token], topn=top_n):\n",
    "\n",
    "        print(f'{word:20} {round(similarity, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft            0.601\n",
      "server               0.595\n",
      "computer             0.594\n",
      "device               0.59\n",
      "pc                   0.578\n",
      "version_of           0.575\n",
      "digital              0.565\n",
      "product              0.551\n",
      "intel                0.551\n",
      "wireless             0.543\n"
     ]
    }
   ],
   "source": [
    "# words similar in context to \"software\"\n",
    "get_related_terms(\"software\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yard                 0.792\n",
      "victory_over         0.766\n",
      "minute               0.703\n",
      "throw                0.684\n",
      "defeat               0.682\n",
      "win_over             0.627\n",
      "sports_network       0.613\n",
      "beat                 0.602\n",
      "point                0.59\n",
      "night                0.58\n"
     ]
    }
   ],
   "source": [
    "get_related_terms(\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "near                 0.583\n",
      "new_york_reuters     0.548\n",
      "send                 0.545\n",
      "japanese             0.52\n",
      "trade                0.513\n",
      "rebel                0.507\n",
      "unit                 0.506\n",
      "target               0.504\n",
      "force                0.493\n",
      "global               0.491\n"
     ]
    }
   ],
   "source": [
    "get_related_terms(\"tokyo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I can add or subtract word embeddings:**\n",
    "- Provide a set of words or phrases that you'd like to add or subtract.\n",
    "- Look up the vectors that represent those terms in the word vector model.\n",
    "- Add and subtract those vectors to produce a new, combined vector.\n",
    "- Look up the most similar vector(s) to this new, combined vector via cosine similarity.\n",
    "- Return the word(s) associated with the similar vector(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_algebra(add=[], subtract=[], topn=1):\n",
    "    \"\"\"\n",
    "    combine the vectors associated with the words provided\n",
    "    in add= and subtract=, look up the topn most similar\n",
    "    terms to the combined vector, and print the result(s)\n",
    "    \"\"\"\n",
    "    answers = w2v.wv.most_similar(positive=add, negative=subtract, topn=topn) # get the resultant word embedding\n",
    "    \n",
    "    for term, similarity in answers:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokyo\n"
     ]
    }
   ],
   "source": [
    "word_algebra(add=['trade', 'new_york'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network\n"
     ]
    }
   ],
   "source": [
    "word_algebra(add=['computer', 'wireless'], subtract=['system'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Visualization with t-SNE\n",
    "- t-Distributed Stochastic Neighbor Embedding, or t-SNE for short, is a dimensionality reduction technique to assist with visualizing high-dimensional datasets. It attempts to map high-dimensional data onto a low two- or three-dimensional representation such that the relative distances between points are preserved as closely as possible in both high-dimensional and low-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn has a TSNE class, we will use that implementation of t-sne\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our input for t-SNE will be the DataFrame of word vectors we created before. But beore that, we will:**\n",
    "- Drop stopwords — it's probably not too interesting to visualize the, of, or, and so on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>-0.022861</td>\n",
       "      <td>-0.050449</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.035906</td>\n",
       "      <td>0.096397</td>\n",
       "      <td>-0.034569</td>\n",
       "      <td>0.135193</td>\n",
       "      <td>0.068926</td>\n",
       "      <td>-0.020791</td>\n",
       "      <td>-0.061270</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013287</td>\n",
       "      <td>-0.100581</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>-0.050550</td>\n",
       "      <td>0.023841</td>\n",
       "      <td>0.087962</td>\n",
       "      <td>0.017231</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.013007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>0.039907</td>\n",
       "      <td>-0.026529</td>\n",
       "      <td>0.048099</td>\n",
       "      <td>-0.049626</td>\n",
       "      <td>0.114245</td>\n",
       "      <td>-0.084603</td>\n",
       "      <td>-0.062243</td>\n",
       "      <td>0.019767</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>-0.024371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072619</td>\n",
       "      <td>-0.117317</td>\n",
       "      <td>-0.022663</td>\n",
       "      <td>-0.022308</td>\n",
       "      <td>-0.037683</td>\n",
       "      <td>0.077738</td>\n",
       "      <td>0.077305</td>\n",
       "      <td>-0.045504</td>\n",
       "      <td>-0.005399</td>\n",
       "      <td>-0.119434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap</th>\n",
       "      <td>0.033210</td>\n",
       "      <td>0.013962</td>\n",
       "      <td>-0.063282</td>\n",
       "      <td>0.082089</td>\n",
       "      <td>-0.033795</td>\n",
       "      <td>-0.086432</td>\n",
       "      <td>-0.101568</td>\n",
       "      <td>-0.060251</td>\n",
       "      <td>0.016080</td>\n",
       "      <td>-0.090278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032503</td>\n",
       "      <td>0.004069</td>\n",
       "      <td>-0.017299</td>\n",
       "      <td>0.011773</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>0.078769</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.135862</td>\n",
       "      <td>0.022339</td>\n",
       "      <td>-0.077558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reuters</th>\n",
       "      <td>-0.035826</td>\n",
       "      <td>-0.120221</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>0.050515</td>\n",
       "      <td>-0.033914</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>-0.061435</td>\n",
       "      <td>-0.067835</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>-0.093429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015985</td>\n",
       "      <td>-0.075737</td>\n",
       "      <td>-0.038027</td>\n",
       "      <td>-0.056025</td>\n",
       "      <td>-0.032797</td>\n",
       "      <td>0.113574</td>\n",
       "      <td>-0.008005</td>\n",
       "      <td>-0.163231</td>\n",
       "      <td>0.056703</td>\n",
       "      <td>-0.078024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>win</th>\n",
       "      <td>0.021388</td>\n",
       "      <td>0.031692</td>\n",
       "      <td>-0.012135</td>\n",
       "      <td>0.066727</td>\n",
       "      <td>0.079087</td>\n",
       "      <td>-0.052649</td>\n",
       "      <td>-0.011732</td>\n",
       "      <td>-0.106668</td>\n",
       "      <td>-0.040447</td>\n",
       "      <td>-0.039560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047772</td>\n",
       "      <td>-0.129981</td>\n",
       "      <td>0.052506</td>\n",
       "      <td>0.026554</td>\n",
       "      <td>-0.018875</td>\n",
       "      <td>0.082755</td>\n",
       "      <td>0.032691</td>\n",
       "      <td>-0.056480</td>\n",
       "      <td>-0.069349</td>\n",
       "      <td>0.018326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6    \\\n",
       "company -0.022861 -0.050449  0.001595  0.035906  0.096397 -0.034569  0.135193   \n",
       "new      0.039907 -0.026529  0.048099 -0.049626  0.114245 -0.084603 -0.062243   \n",
       "ap       0.033210  0.013962 -0.063282  0.082089 -0.033795 -0.086432 -0.101568   \n",
       "reuters -0.035826 -0.120221 -0.046875  0.050515 -0.033914  0.000618 -0.061435   \n",
       "win      0.021388  0.031692 -0.012135  0.066727  0.079087 -0.052649 -0.011732   \n",
       "\n",
       "              7         8         9    ...       290       291       292  \\\n",
       "company  0.068926 -0.020791 -0.061270  ... -0.013287 -0.100581  0.022391   \n",
       "new      0.019767  0.012817 -0.024371  ...  0.072619 -0.117317 -0.022663   \n",
       "ap      -0.060251  0.016080 -0.090278  ...  0.032503  0.004069 -0.017299   \n",
       "reuters -0.067835  0.005880 -0.093429  ...  0.015985 -0.075737 -0.038027   \n",
       "win     -0.106668 -0.040447 -0.039560  ...  0.047772 -0.129981  0.052506   \n",
       "\n",
       "              293       294       295       296       297       298       299  \n",
       "company -0.050550  0.023841  0.087962  0.017231  0.007809  0.009132  0.013007  \n",
       "new     -0.022308 -0.037683  0.077738  0.077305 -0.045504 -0.005399 -0.119434  \n",
       "ap       0.011773 -0.001047  0.078769  0.004370 -0.135862  0.022339 -0.077558  \n",
       "reuters -0.056025 -0.032797  0.113574 -0.008005 -0.163231  0.056703 -0.078024  \n",
       "win      0.026554 -0.018875  0.082755  0.032691 -0.056480 -0.069349  0.018326  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_input = (word_vectors_df.drop(nlp.Defaults.stop_words, errors='ignore')) #drop stopwords from the word embeddings\n",
    "tsne_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02286081, -0.0504493 ,  0.00159523, ...,  0.00780879,\n",
       "         0.00913223,  0.01300692],\n",
       "       [ 0.03990682, -0.02652893,  0.0480987 , ..., -0.04550416,\n",
       "        -0.00539919, -0.11943393],\n",
       "       [ 0.03321031,  0.01396197, -0.06328236, ..., -0.13586223,\n",
       "         0.0223388 , -0.07755819],\n",
       "       ...,\n",
       "       [ 0.04403977, -0.01266175,  0.02819777, ..., -0.03828754,\n",
       "         0.03176774,  0.11160297],\n",
       "       [-0.04722004, -0.00576312,  0.04699276, ...,  0.01243058,\n",
       "         0.00827442,  0.02789558],\n",
       "       [-0.05407192, -0.10239225,  0.02623746, ...,  0.01198477,\n",
       "         0.0012576 ,  0.04053189]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_input.values # array of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_filepath = os.path.join(scratch_directory, 'tsne_model')\n",
    "tsne_vectors_filepath = os.path.join(scratch_directory, 'tsne_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute = True\n",
    "if execute:\n",
    "    # init the tsne object\n",
    "    tsne=TSNE() \n",
    "    tsne_vectors = tsne.fit_transform(tsne_input.values) # array of 300-D word embedding arrays as input\n",
    "    \n",
    "    with open(tsne_filepath, 'wb') as f:\n",
    "        pickle.dump(tsne, f) # save the tsne model\n",
    "        pd.np.save(tsne_vectors_filepath, tsne_vectors) # save the transformed tsne_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsne_vectors\n",
    "with open(tsne_filepath, 'rb') as f:\n",
    "    tsne = pickle.load(f) # get the tsne model\n",
    "\n",
    "tsne_vectors = pd.np.load(tsne_vectors_filepath) # get the tsne vectors    \n",
    "\n",
    "# put tsne vectors into a DataFrame\n",
    "tsne_vectors = pd.DataFrame(\n",
    "    tsne_vectors,\n",
    "    index=pd.Index(tsne_input.index), # row index are the tokens\n",
    "    columns=['x_coord', 'y_coord']  # each token nly have 2 dimensions now \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>14.128726</td>\n",
       "      <td>-12.615202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>13.363648</td>\n",
       "      <td>-6.801100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap</th>\n",
       "      <td>1.764820</td>\n",
       "      <td>17.256807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reuters</th>\n",
       "      <td>-0.102177</td>\n",
       "      <td>-20.531179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>win</th>\n",
       "      <td>9.933373</td>\n",
       "      <td>12.300522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x_coord    y_coord\n",
       "company  14.128726 -12.615202\n",
       "new      13.363648  -6.801100\n",
       "ap        1.764820  17.256807\n",
       "reuters  -0.102177 -20.531179\n",
       "win       9.933373  12.300522"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_vectors.head()  # tsne vectors are 2-D represnetations of the 300D word-embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>14.128726</td>\n",
       "      <td>-12.615202</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>13.363648</td>\n",
       "      <td>-6.801100</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap</th>\n",
       "      <td>1.764820</td>\n",
       "      <td>17.256807</td>\n",
       "      <td>ap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reuters</th>\n",
       "      <td>-0.102177</td>\n",
       "      <td>-20.531179</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>win</th>\n",
       "      <td>9.933373</td>\n",
       "      <td>12.300522</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x_coord    y_coord     word\n",
       "company  14.128726 -12.615202  company\n",
       "new      13.363648  -6.801100      new\n",
       "ap        1.764820  17.256807       ap\n",
       "reuters  -0.102177 -20.531179  reuters\n",
       "win       9.933373  12.300522      win"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_vectors['word'] = tsne_vectors.index \n",
    "tsne_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Plotting Word Vectors with Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add our DataFrame as a ColumnDataSource for Bokeh\n",
    "plot_data = ColumnDataSource(tsne_vectors)\n",
    "\n",
    "# create the plot and configure the\n",
    "# title, dimensions, and tools\n",
    "tsne_plot = figure(\n",
    "    title='t-SNE Word Embeddings',\n",
    "    plot_width=800,\n",
    "    plot_height=800,\n",
    "    tools=(\n",
    "        'pan, wheel_zoom, box_zoom,'\n",
    "        'box_select, reset'\n",
    "        ),\n",
    "    active_scroll='wheel_zoom'\n",
    "    )\n",
    "\n",
    "# add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools(\n",
    "    HoverTool(tooltips = '@word')\n",
    "    )\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(\n",
    "    'x_coord',\n",
    "    'y_coord',\n",
    "    source=plot_data,\n",
    "    color='blue',\n",
    "    line_alpha=0.2,\n",
    "    fill_alpha=0.1,\n",
    "    size=10,\n",
    "    hover_line_color='black'\n",
    "    )\n",
    "\n",
    "# configure visual elements of the plotc\n",
    "tsne_plot.title.text_font_size = value('16pt')\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "# engage!\n",
    "show(tsne_plot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Categorization with spaCy's textcat\n",
    "\n",
    "- https://www.kaggle.com/poonaml/text-classification-using-spacy\n",
    "- https://datascience.stackexchange.com/questions/55896/how-to-train-a-spacy-model-for-text-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I need to wrangle the data to a form that is preferred by spAcy textcat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\": \"Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",\"annotation\":{\"notes\":\"\",\"label\":[\"Business\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1521027375000,\"last_updated_at\":1521027375000,\"sec_taken\":0,\"last_updated_by\":\"nlYZXxNBQefF2u9VX52CdONFp0C3\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"news_article_data.json\") as file:\n",
    "    first_record = file.readline()\n",
    "\n",
    "print(first_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business', 'SciTech', 'Sports', 'World'}"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of headlines that belong to one of the four classes\n",
    "business_headlines = [] # a list of tuples of text and a dict of labels\n",
    "scitech_headlines = []\n",
    "sports_headlines = []\n",
    "world_headlines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists that contain the text and the labels from all the headlines\n",
    "all_content = []\n",
    "all_type = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news_article_data.json') as file:\n",
    "    for i, headline in enumerate(file):\n",
    "        headline = json.loads(headline) # convert to dict\n",
    "        \n",
    "        # populate text and label from all headlines\n",
    "        all_content.append(headline['content'])\n",
    "        all_type.append(headline['annotation']['label'][0])\n",
    "    \n",
    "        # populate the list of (text, label) tuples in a format that is preferred by spaCy textcat\n",
    "        if headline['annotation']['label'][0] ==\"Business\":\n",
    "            business_headlines.append((\n",
    "            headline['content'], # text\n",
    "            {'cats': {'business': 1.0, 'scitech': 0.0, 'sports': 0.0, 'world': 0.0}} # labels\n",
    "            ))\n",
    "            continue\n",
    "            \n",
    "        if headline['annotation']['label'][0] == \"SciTech\":\n",
    "            scitech_headlines.append((\n",
    "            headline['content'], # text\n",
    "            {'cats': {'business': 0.0, 'scitech': 1.0, 'sports': 0.0, 'world': 0.0}} # labels\n",
    "            ))\n",
    "            continue\n",
    "            \n",
    "        if headline['annotation']['label'][0] == \"Sports\":\n",
    "            sports_headlines.append((\n",
    "            headline['content'], # text\n",
    "            {'cats': {'business': 0.0, 'scitech': 0.0, 'sports': 1.0, 'world': 0.0}} # labels\n",
    "            ))\n",
    "            continue\n",
    "            \n",
    "        if headline['annotation']['label'][0] == \"World\":\n",
    "            world_headlines.append((\n",
    "            headline['content'], # text\n",
    "            {'cats': {'business': 0.0, 'scitech': 0.0, 'sports': 0.0, 'world': 1.0}} # labels\n",
    "            ))\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a look at a headline in spaCy's preferred format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters - Apparel retailers are hoping their\\back-to-school fashions will make the grade among\\style-conscious teens and young adults this fall, but it could\\be a tough sell, with students and parents keeping a tighter\\hold on their wallets.\n",
      "\n",
      "{'cats': {'business': 1.0, 'scitech': 0.0, 'sports': 0.0, 'world': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "headline_text, headline_label = business_headlines[1]\n",
    "print(headline_text)\n",
    "print('')\n",
    "print(headline_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.\n",
      "\n",
      "{'cats': {'business': 0.0, 'scitech': 1.0, 'sports': 0.0, 'world': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "headline_text, headline_label = scitech_headlines[1]\n",
    "print(headline_text)\n",
    "print('')\n",
    "print(headline_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOXBOROUGH -- Looking at his ridiculously developed upper body, with huge biceps and hardly an ounce of fat, it's easy to see why Ty Law, arguably the best cornerback in football, chooses physical play over finesse. That's not to imply that he's lacking a finesse component, because he can shut down his side of the field much as Deion Sanders ...\n",
      "\n",
      "{'cats': {'business': 0.0, 'scitech': 0.0, 'sports': 1.0, 'world': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "headline_text, headline_label = sports_headlines[1]\n",
    "print(headline_text)\n",
    "print('')\n",
    "print(headline_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP - The man who claims Gov. James E. McGreevey sexually harassed him was pushing for a cash settlement of up to  #36;50 million before the governor decided to announce that he was gay and had an extramarital affair, sources told The Associated Press.\n",
      "\n",
      "{'cats': {'business': 0.0, 'scitech': 0.0, 'sports': 0.0, 'world': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "headline_text, headline_label = world_headlines[1]\n",
    "print(headline_text)\n",
    "print('')\n",
    "print(headline_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900, 1900, 1900, 1900)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(business_headlines), len(scitech_headlines), len(sports_headlines), len(world_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In addition to using the original headline text as features, we also want to try using the \"transformed\" text as features (tokenized, lemmatized, stopwords and non-alphanumeric characters removed, bigrams and trigrams identified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrangle the transformed text in a format that is preferrable by textcat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the processed text\n",
    "transformed_content = []\n",
    "with open(headlines_trigrams_filepath) as file:\n",
    "    for headline in file:\n",
    "        transformed_content.append(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df = pd.DataFrame(list(zip(all_content, transformed_content, all_type)), columns = ['content', 'transformed', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>transformed</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "      <td>union represent worker turner newall disappoin...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "      <td>toronto canada rocketeer compete million_ansar...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "      <td>ap company found chemistry researcher universi...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "      <td>ap barely dawn mike fitzpatrick start shift bl...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "      <td>ap southern_california smog fighting agency em...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Unions representing workers at Turner   Newall...   \n",
       "1  SPACE.com - TORONTO, Canada -- A second\\team o...   \n",
       "2  AP - A company founded by a chemistry research...   \n",
       "3  AP - It's barely dawn when Mike Fitzpatrick st...   \n",
       "4  AP - Southern California's smog-fighting agenc...   \n",
       "\n",
       "                                         transformed      type  \n",
       "0  union represent worker turner newall disappoin...  Business  \n",
       "1  toronto canada rocketeer compete million_ansar...   SciTech  \n",
       "2  ap company found chemistry researcher universi...   SciTech  \n",
       "3  ap barely dawn mike fitzpatrick start shift bl...   SciTech  \n",
       "4  ap southern_california smog fighting agency em...   SciTech  "
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df[\"transformed\"] = headline_df[\"transformed\"].apply(lambda x: x.replace('\\n',''))  # remove \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get DataFrames of transformed headline texts for each class\n",
    "t_business_df = headline_df.loc[headline_df.type==\"Business\", [\"transformed\", \"type\"]]\n",
    "t_scitech_df = headline_df.loc[headline_df.type==\"SciTech\",  [\"transformed\", \"type\"]]\n",
    "t_sports_df = headline_df.loc[headline_df.type==\"Sports\",  [\"transformed\", \"type\"]]\n",
    "t_world_df = headline_df.loc[headline_df.type==\"World\",  [\"transformed\", \"type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    " # a list of tuples that contains the transformed text and its associated label in spaCy textcat's preferrable format\n",
    "t_business_headlines = [] \n",
    "t_scitech_headlines = []\n",
    "t_sports_headlines = []\n",
    "t_world_headlines = []\n",
    "\n",
    "for row in t_business_df.values.tolist():\n",
    "    t_business_headlines.append((row[0], # text\n",
    "                                 {'cats': {'business': 1.0, 'scitech': 0.0, 'sports': 0.0, 'world': 0.0}} )) # labels  \n",
    "        \n",
    "for row in t_scitech_df.values.tolist():\n",
    "    t_scitech_headlines.append((row[0], # text\n",
    "                                 {'cats': {'business': 0.0, 'scitech': 1.0, 'sports': 0.0, 'world': 0.0}} )) # labels  \n",
    "\n",
    "        \n",
    "for row in t_sports_df.values.tolist():\n",
    "    t_sports_headlines.append((row[0], # text\n",
    "                                 {'cats': {'business': 0.0, 'scitech': 0.0, 'sports': 1.0, 'world': 0.0}} )) # labels\n",
    "    \n",
    "        \n",
    "for row in t_world_df.values.tolist():\n",
    "    t_world_headlines.append((row[0], # text\n",
    "                                 {'cats': {'business': 0.0, 'scitech': 0.0, 'sports': 0.0, 'world': 1.0}} )) # labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union represent worker turner newall disappoint talk_with stricken parent firm federal mogul\n",
      "\n",
      "{'cats': {'business': 1.0, 'scitech': 0.0, 'sports': 0.0, 'world': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# take a look at a headline\n",
    "text, cat = t_business_headlines[0]\n",
    "print(text)\n",
    "print('')\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toronto canada rocketeer compete million_ansari_x_prize contest fund suborbital space flight officially announce date manned rocket\n",
      "\n",
      "{'cats': {'business': 0.0, 'scitech': 1.0, 'sports': 0.0, 'world': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "text, cat = t_scitech_headlines[0]\n",
    "print(text)\n",
    "print('')\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "michael_phelps win gold_medal 400 individual medley set world_record time 4 minute second\n",
      "\n",
      "{'cats': {'business': 0.0, 'scitech': 0.0, 'sports': 1.0, 'world': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "text, cat = t_sports_headlines[0]\n",
    "print(text)\n",
    "print('')\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canadian_press vancouver cp sister man_who die violent confrontation police demand city chief constable resign defend officer involve\n",
      "\n",
      "{'cats': {'business': 0.0, 'scitech': 0.0, 'sports': 0.0, 'world': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "text, cat = t_world_headlines[0]\n",
    "print(text)\n",
    "print('')\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll split the data 50/50 into train and test sets, then randomly shuffle each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = business_headlines[:int(len(business_headlines) / 2)] + scitech_headlines[:int(len(scitech_headlines) / 2)] + sports_headlines[:int(len(sports_headlines) / 2)] + world_headlines[:int(len(world_headlines) / 2)]\n",
    "test_data = business_headlines[int(len(business_headlines) / 2):] + scitech_headlines[int(len(scitech_headlines) / 2):] + sports_headlines[int(len(sports_headlines) / 2):] + world_headlines[int(len(world_headlines) / 2):] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3800, 3800)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat the same thing for the transformed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_data = t_business_headlines[:int(len(t_business_headlines) / 2)] + t_scitech_headlines[:int(len(t_scitech_headlines) / 2)] + t_sports_headlines[:int(len(t_sports_headlines) / 2)] + t_world_headlines[:int(len(t_world_headlines) / 2)]\n",
    "t_test_data = t_business_headlines[int(len(t_business_headlines) / 2):] + t_scitech_headlines[int(len(t_scitech_headlines) / 2):] + t_sports_headlines[int(len(t_sports_headlines) / 2):] + t_world_headlines[int(len(t_world_headlines) / 2):] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(t_train_data)\n",
    "random.shuffle(t_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3800, 3800)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_train_data), len(t_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we'll create a new textcat model and add it to our existing nlp spaCy pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_pipe_names = nlp.pipe_names\n",
    "original_pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the textcat model, default model that it uses is CNN\n",
    "textcat = nlp.create_pipe(\n",
    "    'textcat',\n",
    "    config={'exclusive_classes': True} # classes are mutually exclusive (binary classification)\n",
    "    )\n",
    "\n",
    "# add labels to the model\n",
    "textcat.add_label('business')\n",
    "textcat.add_label('scitech')\n",
    "textcat.add_label('sports')\n",
    "textcat.add_label('world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner', 'textcat']"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add textcat model to the pipe\n",
    "nlp.add_pipe(textcat)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps us evaluate the performance of our text classification model\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    \n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    \n",
    "    tp = 0.0  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    \n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "\n",
    "        gold = cats[i]['cats'] # get the labels for each headline in the test dataset (ex: {Business:1, SciTech:0, Sports:0, World:0})\n",
    "        \n",
    "        # label and score and the predicted z-score for each label\n",
    "        for label, score in doc.cats.items(): \n",
    "            #print((label, score))\n",
    "            \n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.0\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.0\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    \n",
    "    # Compute the estimated test precision, recall and f1-score (from the validation-set approach)\n",
    "    precision = tp / (tp + fp)  # I use micro-averaging method here\n",
    "    recall = tp / (tp + fn)\n",
    "    \n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We start the training loop for our textcat model, using train_data for training the model and test_data for evaluating its performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "4.799\t0.735\t0.335\t0.460\n",
      "3.058\t0.809\t0.707\t0.755\n",
      "1.779\t0.828\t0.777\t0.801\n",
      "1.156\t0.821\t0.786\t0.803\n",
      "0.709\t0.818\t0.793\t0.805\n",
      "0.487\t0.811\t0.788\t0.799\n",
      "0.381\t0.810\t0.791\t0.800\n",
      "0.227\t0.808\t0.794\t0.801\n",
      "0.189\t0.809\t0.798\t0.803\n",
      "0.196\t0.816\t0.802\t0.809\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(*original_pipe_names): # disable ner, tagger, and parser, leaving only textcat enabled in nlp \n",
    "    \n",
    "    optimizer = nlp.begin_training() # initialize weights in CNN model (the default model of textcat)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
    "    \n",
    "    for i in range(10):\n",
    "        losses = {}\n",
    "\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        random.shuffle(train_data)\n",
    "\n",
    "        batches = minibatch(train_data, size=8)\n",
    "\n",
    "        for batch in batches:\n",
    "            texts, cats = zip(*batch)  # train text and labels\n",
    "            nlp.update(texts, cats, sgd=optimizer, drop=0.2, losses=losses) # update model with new train data\n",
    "\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "\n",
    "            # evaluate model on the test data \n",
    "            test_texts, test_cats = zip(*test_data)\n",
    "            scores = evaluate(nlp.tokenizer, textcat, test_texts, test_cats)\n",
    "\n",
    "        print(\n",
    "            \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table of train loss, and test precision, recall and f-score of each epoch\n",
    "                losses[\"textcat\"],\n",
    "                scores[\"textcat_p\"],\n",
    "                scores[\"textcat_r\"],\n",
    "                scores[\"textcat_f\"],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and assess model using transformed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "2.005\t0.789\t0.741\t0.764\n",
      "0.876\t0.791\t0.757\t0.774\n",
      "0.493\t0.785\t0.763\t0.773\n",
      "0.292\t0.778\t0.763\t0.771\n",
      "0.242\t0.775\t0.761\t0.768\n",
      "0.193\t0.773\t0.761\t0.767\n",
      "0.141\t0.773\t0.762\t0.767\n",
      "0.096\t0.772\t0.764\t0.768\n",
      "0.123\t0.770\t0.761\t0.765\n",
      "0.106\t0.765\t0.756\t0.761\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(*original_pipe_names):\n",
    "    \n",
    "    optimizer = nlp.begin_training()\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
    "    \n",
    "    for i in range(10):\n",
    "        losses = {}\n",
    "\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        random.shuffle(t_train_data)\n",
    "\n",
    "        batches = minibatch(t_train_data, size=8)\n",
    "\n",
    "        for batch in batches:\n",
    "            texts, cats = zip(*batch)\n",
    "            nlp.update(texts, cats, sgd=optimizer, drop=0.2, losses=losses)\n",
    "\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "\n",
    "            # evaluate on the dev data split off in load_data()\n",
    "            test_texts, test_cats = zip(*t_test_data)\n",
    "            scores = evaluate(nlp.tokenizer, textcat, test_texts, test_cats)\n",
    "\n",
    "        print(\n",
    "            \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table\n",
    "                losses[\"textcat\"],\n",
    "                scores[\"textcat_p\"],\n",
    "                scores[\"textcat_r\"],\n",
    "                scores[\"textcat_f\"],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoguhts:\n",
    "- If the lemmatized, cleaned, and transformed text is used to trained the CNN model, its performance is actually worse than if I used the original text.\n",
    "- The train loss decreased at each epoch while the test metrics do not change much, this indicates overfitting.  I might only need 10 epoches since the test metrics do not change much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
